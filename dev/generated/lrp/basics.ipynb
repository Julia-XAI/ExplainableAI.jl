{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic usage of LRP\n",
    "This example will show you best practices for using LRP,\n",
    "building on the basics shown in the *Getting started* section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start out by loading a small convolutional neural network:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ExplainableAI\n",
    "using Flux\n",
    "\n",
    "model = Chain(\n",
    "    Chain(\n",
    "        Conv((3, 3), 3 => 8, relu; pad=1),\n",
    "        Conv((3, 3), 8 => 8, relu; pad=1),\n",
    "        MaxPool((2, 2)),\n",
    "        Conv((3, 3), 8 => 16; pad=1),\n",
    "        BatchNorm(16, relu),\n",
    "        Conv((3, 3), 16 => 8, relu; pad=1),\n",
    "        BatchNorm(8, relu),\n",
    "    ),\n",
    "    Chain(\n",
    "        Flux.flatten,\n",
    "        Dense(2048 => 512, relu),\n",
    "        Dropout(0.5),\n",
    "        Dense(512 => 100, softmax)\n",
    "    ),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model contains two chains: the convolutional layers and the fully connected layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model preparation\n",
    "### Stripping the output softmax\n",
    "When using LRP, it is recommended to explain output logits instead of probabilities.\n",
    "This can be done by stripping the output softmax activation from the model\n",
    "using the `strip_softmax` function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Chain(\n    Conv((3, 3), 3 => 8, relu, pad=1),  \u001b[90m# 224 parameters\u001b[39m\n    Conv((3, 3), 8 => 8, relu, pad=1),  \u001b[90m# 584 parameters\u001b[39m\n    MaxPool((2, 2)),\n    Conv((3, 3), 8 => 16, pad=1),       \u001b[90m# 1_168 parameters\u001b[39m\n    BatchNorm(16, relu),                \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 32\u001b[39m\n    Conv((3, 3), 16 => 8, relu, pad=1),  \u001b[90m# 1_160 parameters\u001b[39m\n    BatchNorm(8, relu),                 \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  ),\n  Chain(\n    Flux.flatten,\n    Dense(2048 => 512, relu),           \u001b[90m# 1_049_088 parameters\u001b[39m\n    Dropout(0.5),\n    Dense(512 => 100),                  \u001b[90m# 51_300 parameters\u001b[39m\n  ),\n) \u001b[90m        # Total: 16 trainable arrays, \u001b[39m1_103_572 parameters,\n\u001b[90m          # plus 4 non-trainable, 48 parameters, summarysize \u001b[39m4.213 MiB."
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "model = strip_softmax(model)"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you don't remove the output softmax,\n",
    "model checks will fail."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Canonizing the model\n",
    "LRP is not invariant to a model's implementation.\n",
    "Applying the `GammaRule` to two linear layers in a row will yield different results\n",
    "than first fusing the two layers into one linear layer and then applying the rule.\n",
    "This fusing is called \"canonization\" and can be done using the `canonize` function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),   \u001b[90m# 1_168 parameters\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 14 trainable arrays, \u001b[39m1_103_540 parameters,\n\u001b[90m          # plus 2 non-trainable, 16 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "model_canonized = canonize(model)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "After canonization, the first `BatchNorm` layer has been fused into the preceding `Conv` layer.\n",
    "The second `BatchNorm` layer wasn't fused\n",
    "since its preceding `Conv` layer has a ReLU activation function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flattening the model\n",
    "ExplainableAI.jl's LRP implementation supports nested Flux Chains and Parallel layers.\n",
    "However, it is recommended to flatten the model before analyzing it.\n",
    "\n",
    "LRP is implemented by first running a forward pass through the model,\n",
    "keeping track of the intermediate activations, followed by a backward pass\n",
    "that computes the relevances.\n",
    "\n",
    "To keep the LRP implementation simple and maintainable,\n",
    "ExplainableAI.jl does not pre-compute \"nested\" activations.\n",
    "Instead, for every internal chain, a new forward pass is run to compute activations.\n",
    "\n",
    "By \"flattening\" a model, this overhead can be avoided.\n",
    "For this purpose, ExplainableAI.jl provides the function `flatten_model`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, pad=1),         \u001b[90m# 1_168 parameters\u001b[39m\n  BatchNorm(16, relu),                  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 32\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 16 trainable arrays, \u001b[39m1_103_572 parameters,\n\u001b[90m          # plus 4 non-trainable, 48 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "model_flat = flatten_model(model)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function is called by default when creating an LRP analyzer.\n",
    "Note that we pass the unflattened model to the analyzer, but `analyzer.model` is flattened:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, pad=1),         \u001b[90m# 1_168 parameters\u001b[39m\n  BatchNorm(16, relu),                  \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 32\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 16 trainable arrays, \u001b[39m1_103_572 parameters,\n\u001b[90m          # plus 4 non-trainable, 48 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "analyzer = LRP(model)\n",
    "analyzer.model"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "If this flattening is not desired, it can be disabled\n",
    "by passing the keyword argument `flatten=false` to the `LRP` constructor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LRP rules\n",
    "By default, the `LRP` constructor will assign the `ZeroRule` to all layers."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  MaxPool((2, 2))                   \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 16, pad=1)      \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  BatchNorm(16, relu)               \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 16 => 8, relu, pad=1)\u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  BatchNorm(8, relu)                \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Flux.flatten                      \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dense(2048 => 512, relu)          \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dropout(0.5)                      \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dense(512 => 100)                 \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "LRP(model)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "This analyzer will return heatmaps that look identical to `InputTimesGradient`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "LRP's strength lies in assigning different rules to different layers,\n",
    "based on their functionality in the neural network[^1].\n",
    "ExplainableAI.jl implements many LRP rules out of the box,\n",
    "but it is also possible to *implement custom rules*.\n",
    "\n",
    "To assign different rules to different layers,\n",
    "use one of the composites presets,\n",
    "or create your own composite, as described in\n",
    "*Assigning rules to layers*."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Composite(\n  GlobalTypeMap(  \u001b[90m# all layers\u001b[39m\n\u001b[94m    Flux.Conv              \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.ConvTranspose     \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.CrossCor          \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.Dense             \u001b[39m => \u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n\u001b[94m    typeof(NNlib.dropout)  \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.AlphaDropout      \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.Dropout           \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.BatchNorm         \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(Flux.flatten)   \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(MLUtils.flatten)\u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(identity)       \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n ),\n  FirstLayerTypeMap(  \u001b[90m# first layer\u001b[39m\n\u001b[94m    Flux.Conv         \u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n\u001b[94m    Flux.ConvTranspose\u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n\u001b[94m    Flux.CrossCor     \u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n ),\n)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "composite = EpsilonPlusFlat() # using composite preset EpsilonPlusFlat"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mFlatRule()\u001b[39m,\n  Conv((3, 3), 8 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  MaxPool((2, 2))                   \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 16, pad=1)      \u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  BatchNorm(16, relu)               \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Conv((3, 3), 16 => 8, relu, pad=1)\u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  BatchNorm(8, relu)                \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Flux.flatten                      \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Dense(2048 => 512, relu)          \u001b[90m => \u001b[39m\u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n  Dropout(0.5)                      \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Dense(512 => 100)                 \u001b[90m => \u001b[39m\u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "LRP(model, composite)"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing layerwise relevances\n",
    "If you are interested in computing layerwise relevances,\n",
    "call `analyze` with an LRP analyzer and the keyword argument\n",
    "`layerwise_relevances=true`.\n",
    "\n",
    "The layerwise relevances can be accessed in the `extras` field\n",
    "of the returned `Explanation`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([6.0835757 101.06753 … -4726.376 2621.1094; 3.7184293 31.364473 … -15991.358 1799.134; … ; 15.348801 -35.77625 … -94.88924 3.6887348; -31.410635 0.000392046 … -22.1984 -9.494499;;; -37.438824 25.092264 … 7049.8745 609.3779; -161.62701 -49.905167 … 2892.7021 10695.654; … ; -124.63376 4.2213907 … -20.314743 -61.87128; -11.541215 -3.6323142 … -2.3679073 -38.91673;;; 32.22077 -12.463497 … 11756.434 -432.7099; -63.59459 32.38293 … 1380.8066 1785.4785; … ; -5.139298 14.26088 … -19.761318 -67.99524; -60.18135 -6.0685835 … -41.19852 -5.99619;;;;], [-8.383205 -6.7385087 … -0.0 0.0; 0.0 0.0 … 0.0 -0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; -17.660366 0.0 … 636.59375 -0.0; -23.93031 0.0 … 0.0 0.0; … ; -1.9732946 -0.0 … -0.0 -0.0; -0.0 -1.4547555 … -0.0 -0.0;;; 16.020035 62.531902 … 8997.275 -1006.3046; 11.971881 -59.370346 … 2702.6716 -132.8796; … ; -14.948251 36.916904 … 0.26344928 4.532276; -0.0 -102.97552 … -14.331663 -18.336082;;; 0.0 0.0 … -822.18164 -0.0; 0.0 -0.0 … 339.25232 0.0; … ; -4.621726 -92.152115 … -18.661661 -0.0; -0.0 -20.546793 … -15.108921 -0.0;;; -9.852241 -14.279871 … 0.0 344.63974; -0.0 -7.8349695 … 181.83247 -2158.0264; … ; -0.0 -19.51203 … 0.0 -1.2623912; -4.3143497 -8.822921 … 1.5534705 -0.0;;; -78.8026 -100.23479 … 3640.6194 7248.5767; -42.01707 139.64024 … -24753.99 1501.6459; … ; -146.86182 -170.56172 … -23.664757 -123.62927; 7.3738627 17.657612 … -44.241238 -20.87881;;; -0.0 7.8413467 … -2411.037 0.0; 0.0 7.111142 … 0.0 86.02797; … ; -0.0 0.0 … -0.0 10.607327; -0.0 -0.0 … 0.0 -0.0;;; 0.0 0.0 … -0.0 -22.184755; -0.0 -0.0 … -0.0 0.0; … ; -0.0 0.0 … 0.0 0.0; 0.0 -0.0 … 0.0 2.470061;;;;], [0.0 0.0 … 0.0 0.0; 0.0 174.7292 … 22355.42 0.0; … ; -145.16054 0.0 … -110.982605 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 19.41643 … 0.0 1793.106; … ; 0.0 21.562737 … 0.0 -146.41805; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 -161.78304 … -2060.9482 0.0; … ; 0.0 -19.370962 … 33.584087 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; -15.556762 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 -77.02771 … 15201.725 0.0; … ; 0.0 -217.1104 … -174.92598 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 -2.3733747 … 0.0 1140.4418; … ; 0.0 -9.278163 … 0.0 68.14412; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 -217.02393; 0.0 -20.50655 … 0.0 0.0; … ; 0.0 0.0 … 0.0 79.979774; 0.0 -33.98299 … 0.0 0.0;;; 26.210363 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;;;], [174.7292 72.59276 … 14237.67 22355.42; -64.99157 -180.83772 … 33340.06 -17587.701; … ; -77.48173 -218.16173 … -183.60599 -362.40622; -145.16054 -81.15579 … -106.26164 -110.982605;;; 19.41643 -14.965037 … -3379.1604 1793.106; 48.97925 31.685764 … 10937.489 20126.084; … ; 78.63927 67.70398 … 45.58545 -152.4756; 21.562737 -71.6947 … -56.7115 -146.41805;;; -161.78304 -124.156525 … -15980.939 -2060.9482; 55.596733 61.949886 … 41614.08 2373.5444; … ; 38.76218 80.492035 … 51.79536 64.435196; -19.370962 -6.315173 … -10.576787 33.584087;;; -0.0 -0.0 … 0.0 -0.0; -94.54713 -0.0 … 0.0 -0.0; … ; -170.54463 -0.0 … -0.0 -0.0; -15.556762 -0.0 … -0.0 -0.0;;; -77.02771 -288.3438 … -21951.473 15201.725; -163.60765 -580.618 … 30012.44 42642.527; … ; -218.61943 -631.09796 … -548.87866 -293.87183; -217.1104 -348.47882 … -280.4376 -174.92598;;; -2.3733747 46.27766 … 14224.571 1140.4418; -23.13588 60.84848 … -1181.7185 -9181.661; … ; -27.702734 44.894817 … 53.50358 22.409428; -9.278163 75.21964 … 62.35601 68.14412;;; -20.50655 -51.593273 … 1556.5226 -217.02393; -95.77164 -25.944353 … 13564.352 -26589.318; … ; -44.39438 -30.163227 … -54.316425 -0.23196357; -33.98299 -1.6789881 … -1.9055322 79.979774;;; 26.210363 0.0 … -0.0 -0.0; 0.0 0.0 … -0.0 -0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;;;], [0.0010787519 0.00025898177 … 0.001044943 -0.0013042415; 0.0008508212 -0.00020112649 … 0.0007868878 -0.00273648; … ; 0.000765391 0.0009851787 … -0.0006139977 -0.0028386163; 0.0118068475 0.0010170359 … 0.00084771786 0.00077743334;;; -0.0077982224 0.04002143 … 0.03204496 0.010817456; 0.0011382117 -0.0008050302 … -0.0060691303 0.002839481; … ; -0.017239083 -0.0074248607 … -0.002870207 -0.00596568; -0.073594876 -0.034282763 … -0.04014191 0.009427103;;; 41.493435 64.66995 … 64.79072 44.347294; 55.704247 69.22651 … -4547.454 76.22088; … ; 39.314007 71.72895 … 67.0427 62.983273; 23.280624 51.61532 … 57.866734 47.91877;;; … ;;; -0.006077489 -2.6066265 … -1.9881505 -3.1361878; -0.482426 -1.0991148 … 179.43059 129.61464; … ; -0.20733164 -0.37178043 … -1.0054007 0.3721997; -0.015802577 -0.05079713 … 5.123174f-5 -0.0032996656;;; 13.88042 -0.17525247 … -6.7089186 -22.580894; 28.43328 11.832531 … -0.0319618 -5.421331; … ; 28.00023 3.1069977 … 14.802242 -18.888502; 24.72314 24.222912 … 34.813736 6.7965083;;; 0.19432701 0.35891965 … 0.22179483 0.5317331; -0.21867776 -0.62420535 … -56.92814 -0.24333335; … ; -0.3553055 -0.3753252 … -0.5939803 -0.088563554; -0.49315342 2.1062589 … -0.29298988 -0.59402126;;;;], [0.0 0.0 … 0.0 0.0; 0.0 0.0 … -0.0 0.0; … ; 0.0 0.0 … 0.0 -0.0; 0.058652423 0.0 … 0.0 -0.0;;; -0.0 -0.0 … -0.0 -0.0; -0.015512268 -0.0 … -0.0 0.0; … ; -0.10824135 -0.0 … -0.0 -0.0; -0.23383467 -0.14887622 … -0.1623472 -0.04428945;;; -0.17920984 -0.68975705 … -0.3869355 -0.1550304; -0.55981535 -1.4054931 … -67.13674 105.76622; … ; -0.3798075 -1.3419513 … -1.5683614 -0.25513873; -0.2210249 -0.6461109 … -0.7446575 -0.075611524;;; … ;;; -0.0 -0.509745 … -0.33768332 -0.55897176; -0.0 -0.08132349 … 55.79785 74.18179; … ; -0.0 -0.062422805 … -0.040844895 0.07180281; 0.0 0.11579202 … 0.0019197102 0.035776604;;; 0.16984856 0.0 … -0.0 -0.0; 0.49854237 0.10003582 … -0.0 0.0; … ; 0.472157 0.034372933 … 0.18035391 -0.0; 0.18772395 0.14708103 … 0.23797482 -0.034319185;;; 0.0 0.0 … 0.0 0.0; 0.10815874 0.37199858 … -12.465197 -8.576349; … ; 0.16851564 0.20402938 … 0.3394924 0.016694358; 0.090618424 0.091553815 … 0.14331271 -0.0013854972;;;;], [-0.0 -0.0 … -0.0 -0.0; 1.2058796f-5 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0;;; 0.0 -0.024374165 … 0.009680698 0.007882664; 0.0 0.0 … 0.0015622992 0.0021072212; … ; 0.0 0.0 … 0.00073363475 -0.00025866824; 0.0 0.0 … 0.00043451638 0.0008960801;;; -0.7893255 -0.5873329 … -0.3660127 -0.0; -1.7327533 -2.5527585 … -2.4571354 -0.8841951; … ; -1.6425068 -1.9656131 … -1.9604167 -0.78701097; -1.2129288 -1.2404475 … -1.4060721 -0.5665387;;; -0.0 -0.0 … -0.0 -0.0012566122; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 0.00027440372;;; -0.008288511 -0.02564824 … -0.014574273 0.0; -0.008645106 0.008672354 … -0.0014950474 0.0; … ; 0.0054608798 0.0 … 0.024162402 0.0; 0.0 0.0 … 0.0 0.0;;; -0.0 0.0009497078 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0;;; 0.0 0.0 … 0.0 0.0; 0.008619467 0.0014662143 … 0.0 0.012551319; … ; 0.0091579305 0.078137845 … 0.055736013 0.003347887; 0.014569762 0.0032932656 … 0.02567784 0.026876686;;; 0.0 3.1984622f-5 … 0.0014916973 0.028804004; 0.0 0.002660323 … 0.007572709 -0.009657869; … ; 0.0 -0.028636603 … 0.041136935 0.0023356287; 0.0 0.0 … 0.0 0.0;;;;], [0.0 -0.0 … -0.0 -0.0; 1.2059086f-5 0.0 … -0.0 -0.0; … ; 0.0 -0.0 … 0.0 0.0; 0.0 0.0 … -0.0 -0.0;;; 0.0 -0.0028232387 … 0.0026610207 0.0016907892; 0.0 0.0 … 0.0058377683 -0.0055976617; … ; 0.0 0.0 … -7.6133045f-5 -0.0020048087; -0.0 -0.0 … 0.0004157621 0.0030487124;;; -0.012164672 -0.00094542216 … -0.0038084998 0.0; 0.012097856 0.017150102 … 0.0022346408 0.01930629; … ; 0.027106559 0.00598604 … 0.00093488937 -0.011677266; -0.003253973 0.025994921 … -0.013853342 -0.009943815;;; 0.0 0.0 … -0.0 -0.001551634; -0.0 -0.0 … -0.0 -0.0; … ; 0.0 0.0 … -0.0 0.0; -0.0 0.0 … 0.0 0.00020081167;;; -0.010238682 -0.027090345 … -0.016208163 -0.0; -0.006483242 0.002772703 … -0.0007109217 0.0; … ; 0.0022693141 -0.0 … 0.004568885 -0.0; 0.0 0.0 … 0.0 -0.0;;; 0.0 0.001670653 … 0.0 0.0; 0.0 0.0 … -0.0 0.0; … ; -0.0 -0.0 … -0.0 -0.0; 0.0 -0.0 … -0.0 0.0;;; 0.0 0.0 … -0.0 -0.0; -0.00069817156 6.521566f-5 … 0.0 -0.0017087172; … ; 0.0019152932 0.017939607 … 0.010385642 -0.009975458; 0.0025686733 -0.008601404 … -0.0035388088 0.0007046278;;; 0.0 1.4952236f-5 … 0.0026068897 0.0063895714; -0.0 0.00036586286 … 0.001109913 -0.0037828973; … ; -0.0 -0.0053136884 … 0.021101967 -0.0014377247; 0.0 -0.0 … -0.0 0.0;;;;], Float32[0.0; 1.2059086f-5; … ; -0.0014377247; 0.0;;], Float32[0.051761985; 0.001321845; … ; -0.0; -0.028343294;;], Float32[0.051761985; 0.001321845; … ; -0.0; -0.028343294;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;])"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "input = rand(Float32, 32, 32, 3, 1) # dummy input for our convolutional neural network\n",
    "\n",
    "expl = analyze(input, analyzer; layerwise_relevances=true)\n",
    "expl.extras.layerwise_relevances"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the layerwise relevances are only kept for layers in the outermost `Chain` of the model.\n",
    "When using our unflattened model, we only obtain three layerwise relevances,\n",
    "one for each chain in the model and the output relevance:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "([-1.1828043 -0.5146105 … -7.2853165 4.0733533; -0.01651892 0.38497776 … -27.326485 2.7742867; … ; 0.10874569 0.062238906 … 0.48257518 -0.19437595; -0.41963062 -0.0076653487 … -0.031116791 -0.28727654;;; 0.337978 0.020814802 … 11.564356 0.25502783; -0.7519693 -0.22657557 … 4.2886243 17.080915; … ; -2.0864449 0.022563817 … 0.059388988 -0.82922727; -0.2963327 -0.020248784 … 0.010476462 -0.31385106;;; -0.7192914 0.078907266 … 17.898315 -1.0163828; -0.13391703 -0.7938652 … 2.1644552 2.968717; … ; -0.36246592 -1.6220001 … -0.87313825 -0.30269152; -0.6182218 -0.064212136 … -0.31559542 0.17359598;;;;], [0.0 -0.0 … -0.0 -0.0; 0.0 0.0 … -0.0 -0.0; … ; 0.0 -0.0 … 0.0 0.0; 0.0 0.0 … -0.0 -0.0;;; 0.0 -0.0009444967 … 0.00326688 0.0018390116; 0.0 0.0 … 0.005368177 -0.0040899073; … ; 0.0 0.0 … -8.052066f-5 -0.0024958164; -0.0 -0.0 … 0.0 0.0018604868;;; -0.008773063 9.804656f-5 … -0.0009650733 -0.0; 0.015490826 0.015252489 … 0.011222648 0.020031318; … ; 0.024869507 0.014143492 … 0.00311105 -0.0128324805; -0.0048603853 0.020994417 … -0.008136188 -0.0076352744;;; 0.0 0.0 … -0.0 -0.0012100956; -0.0 -0.0 … -0.0 -0.0; … ; 0.0 0.0 … -0.0 -0.0; -0.0 0.0 … 0.0 7.643059f-5;;; -0.014678045 -0.033615883 … -0.016839584 -0.0; -0.005296903 0.0016922202 … -0.0009838123 -0.0; … ; 0.0032853424 -0.0 … 0.0034592922 -0.0; 0.0 0.0 … 0.0 -0.0;;; 0.0 0.0016428964 … 0.0 0.0; 0.0 0.0 … -0.0 0.0; … ; 0.0 -0.0 … -0.0 -0.0; 0.0 -0.0 … -0.0 0.0;;; 0.0 0.0 … -0.0 -0.0; -1.9356272f-5 -0.0 … 0.0 -0.00022924153; … ; 0.0016639868 0.020324728 … 0.012490461 -0.009521527; 0.0014748124 -0.0050077955 … -0.004797944 0.002744675;;; 0.0 -0.0 … 0.0028212145 0.0062024156; -0.0 0.00037579084 … 0.0020380707 -0.0036670172; … ; -0.0 -0.0061384155 … 0.022307679 -0.0005777449; 0.0 -0.0 … -0.0 0.0;;;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "analyzer = LRP(model; flatten=false) # use unflattened model\n",
    "\n",
    "expl = analyze(input, analyzer; layerwise_relevances=true)\n",
    "expl.extras.layerwise_relevances"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance tips\n",
    "### Using LRP with a GPU\n",
    "Like all other analyzers, LRP can be used on GPUs.\n",
    "Follow the instructions on *GPU support*.\n",
    "\n",
    "### Using LRP without a GPU\n",
    "Using Julia's package extension mechanism,\n",
    "ExplainableAI.jl's LRP implementation can optionally make use of\n",
    "[Tullio.jl](https://github.com/mcabbott/Tullio.jl) and\n",
    "[LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "for faster LRP rules on dense layers.\n",
    "\n",
    "This only requires loading the packages before loading ExplainableAI.jl:\n",
    "```julia\n",
    "using LoopVectorization, Tullio\n",
    "using ExplainableAI\n",
    "```\n",
    "\n",
    "[^1]: G. Montavon et al., [Layer-Wise Relevance Propagation: An Overview](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_10)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
