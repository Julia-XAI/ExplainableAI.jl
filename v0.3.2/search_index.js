var documenterSearchIndex = {"docs":
[{"location":"api/#Basics","page":"API Reference","title":"Basics","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"All methods in ExplainableAI.jl work by calling analyze on an input and an analyzer:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"analyze\nheatmap","category":"page"},{"location":"api/#ExplainableAI.analyze","page":"API Reference","title":"ExplainableAI.analyze","text":"analyze(input, method)\nanalyze(input, method, neuron_selection)\n\nReturn raw classifier output and explanation. If neuron_selection is specified, the explanation will be calculated for that neuron. Otherwise, the output neuron with the highest activation is automatically chosen.\n\nKeyword arguments\n\nadd_batch_dim: add batch dimension to the input without allocating. Default is false.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.heatmap","page":"API Reference","title":"ExplainableAI.heatmap","text":"heatmap(expl::Explanation; kwargs...)\nheatmap(attr::AbstractArray; kwargs...)\n\nheatmap(input, analyzer::AbstractXAIMethod)\nheatmap(input, analyzer::AbstractXAIMethod, neuron_selection::Int)\n\nVisualize explanation. Assumes Flux's WHCN convention (width, height, color channels, batch size).\n\nKeyword arguments\n\ncs::ColorScheme: ColorScheme that is applied.   When calling heatmap with an Explanation or analyzer, the method default is selected.   When calling heatmap with an array, the default is ColorSchemes.bwr.\nreduce::Symbol: How the color channels are reduced to a single number to apply a colorscheme.   The following methods can be selected, which are then applied over the color channels   for each \"pixel\" in the attribution:\n:sum: sum up color channels\n:norm: compute 2-norm over the color channels\n:maxabs: compute maximum(abs, x) over the color channels in\nWhen calling heatmap with an Explanation or analyzer, the method default is selected.   When calling heatmap with an array, the default is :sum.\nnormalize::Symbol: How the color channel reduced heatmap is normalized before the colorscheme is applied.   Can be either :extrema or :centered.   When calling heatmap with an Explanation or analyzer, the method default is selected.   When calling heatmap with an array, the default for use with the bwr colorscheme is :centered.\npermute::Bool: Whether to flip W&H input channels. Default is true.\nunpack_singleton::Bool: When heatmapping a batch with a single sample, setting unpack_singleton=true   will return an image instead of an Vector containing a single image.\n\nNote: these keyword arguments can't be used when calling heatmap with an analyzer.\n\n\n\n\n\n","category":"function"},{"location":"api/#Analyzers","page":"API Reference","title":"Analyzers","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"LRP\nGradient\nInputTimesGradient\nSmoothGrad","category":"page"},{"location":"api/#ExplainableAI.LRP","page":"API Reference","title":"ExplainableAI.LRP","text":"LRP(c::Chain, r::AbstractLRPRule)\nLRP(c::Chain, rs::AbstractVector{<:AbstractLRPRule})\n\nAnalyze model by applying Layer-Wise Relevance Propagation.\n\nKeyword arguments\n\nskip_checks::Bool: Skip checks whether model is compatible with LRP and contains output softmax. Default is false.\nverbose::Bool: Select whether the model checks should print a summary on failure. Default is true.\n\nReferences\n\n[1] G. Montavon et al., Layer-Wise Relevance Propagation: An Overview [2] W. Samek et al., Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.Gradient","page":"API Reference","title":"ExplainableAI.Gradient","text":"Gradient(model)\n\nAnalyze model by calculating the gradient of a neuron activation with respect to the input.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.InputTimesGradient","page":"API Reference","title":"ExplainableAI.InputTimesGradient","text":"InputTimesGradient(model)\n\nAnalyze model by calculating the gradient of a neuron activation with respect to the input. This gradient is then multiplied element-wise with the input.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.SmoothGrad","page":"API Reference","title":"ExplainableAI.SmoothGrad","text":"SmoothGrad(analyzer, [n=50, std=0.1, rng=GLOBAL_RNG])\nSmoothGrad(analyzer, [n=50, distribution=Normal(0, σ²=0.01), rng=GLOBAL_RNG])\n\nAnalyze model by calculating a smoothed sensitivity map. This is done by averaging sensitivity maps of a Gradient analyzer over random samples in a neighborhood of the input, typically by adding Gaussian noise with mean 0.\n\nReferences\n\n[1] Smilkov et al., SmoothGrad: removing noise by adding noise\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API Reference","title":"API Reference","text":"SmoothGrad is a special case of InputAugmentation, which can be applied as a wrapper to any analyzer:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"InputAugmentation","category":"page"},{"location":"api/#LRP","page":"API Reference","title":"LRP","text":"","category":"section"},{"location":"api/#Rules","page":"API Reference","title":"Rules","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"ZeroRule\nGammaRule\nEpsilonRule\nZBoxRule","category":"page"},{"location":"api/#ExplainableAI.ZeroRule","page":"API Reference","title":"ExplainableAI.ZeroRule","text":"ZeroRule()\n\nConstructor for LRP-0 rule. Commonly used on upper layers.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.GammaRule","page":"API Reference","title":"ExplainableAI.GammaRule","text":"GammaRule(; γ=0.25)\n\nConstructor for LRP-γ rule. Commonly used on lower layers.\n\nArguments:\n\nγ: Optional multiplier for added positive weights, defaults to 0.25.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.EpsilonRule","page":"API Reference","title":"ExplainableAI.EpsilonRule","text":"EpsilonRule(; ϵ=1f-6)\n\nConstructor for LRP-ϵ rule. Commonly used on middle layers.\n\nArguments:\n\nϵ: Optional stabilization parameter, defaults to 1f-6.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.ZBoxRule","page":"API Reference","title":"ExplainableAI.ZBoxRule","text":"ZBoxRule()\n\nConstructor for LRP-z^mathcalB-rule. Commonly used on the first layer for pixel input.\n\n\n\n\n\n","category":"type"},{"location":"api/#Custom-rules","page":"API Reference","title":"Custom rules","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"These utilities can be used to define custom rules without writing boilerplate code:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"modify_denominator\nmodify_params\nmodify_layer\nLRP_CONFIG.supports_layer\nLRP_CONFIG.supports_activation","category":"page"},{"location":"api/#ExplainableAI.modify_denominator","page":"API Reference","title":"ExplainableAI.modify_denominator","text":"modify_denominator(rule, d)\n\nFunction that modifies zₖ on the forward pass, e.g. for numerical stability.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.modify_params","page":"API Reference","title":"ExplainableAI.modify_params","text":"modify_params(rule, W, b)\n\nFunction that modifies weights and biases before applying relevance propagation. Returns modified weights and biases as a tuple (ρW, ρb).\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.modify_layer","page":"API Reference","title":"ExplainableAI.modify_layer","text":"modify_layer(rule, layer)\n\nFunction that modifies a layer before applying relevance propagation. Returns a new, modified layer.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.LRP_CONFIG.supports_layer","page":"API Reference","title":"ExplainableAI.LRP_CONFIG.supports_layer","text":"LRP_CONFIG.supports_layer(layer)\n\nCheck whether LRP can be used on a layer or a Chain. To extend LRP to your own layers, define:\n\nLRP_CONFIG.supports_layer(::MyLayer) = true          # for structs\nLRP_CONFIG.supports_layer(::typeof(mylayer)) = true  # for functions\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.LRP_CONFIG.supports_activation","page":"API Reference","title":"ExplainableAI.LRP_CONFIG.supports_activation","text":"LRP_CONFIG.supports_activation(σ)\n\nCheck whether LRP can be used on a given activation function. To extend LRP to your own activation functions, define:\n\nLRP_CONFIG.supports_activation(::typeof(myactivation)) = true  # for functions\nLRP_CONFIG.supports_activation(::MyActivation) = true          # for structs\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"strip_softmax\nflatten_model","category":"page"},{"location":"api/#ExplainableAI.strip_softmax","page":"API Reference","title":"ExplainableAI.strip_softmax","text":"strip_softmax(model)\n\nRemove softmax activation on model output if it exists.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.flatten_model","page":"API Reference","title":"ExplainableAI.flatten_model","text":"flatten_model(c)\n\nFlatten a Flux chain containing Flux chains.\n\n\n\n\n\n","category":"function"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"EditURL = \"https://github.com/adrhill/ExplainableAI.jl/blob/master/docs/literate/advanced_lrp.jl\"","category":"page"},{"location":"generated/advanced_lrp/#Advanced-LRP-usage","page":"Advanced LRP","title":"Advanced LRP usage","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"One of the design goals of ExplainableAI.jl is to combine ease of use and extensibility for the purpose of research.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"This example will show you how to implement custom LRP rules and register custom layers and activation functions. For this purpose, we will quickly load our model from the previous section:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"using ExplainableAI\nusing Flux\nusing MLDatasets\nusing ImageCore\nusing BSON\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model]\n\nindex = 10\nx, y = MNIST.testdata(Float32, index)\ninput = reshape(x, 28, 28, 1, :);\nnothing #hide","category":"page"},{"location":"generated/advanced_lrp/#Custom-LRP-composites","page":"Advanced LRP","title":"Custom LRP composites","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Instead of creating an LRP-analyzer from a single rule (e.g. LRP(model, GammaRule())), we can also assign rules to each layer individually. For this purpose, we create an array of rules that matches the length of the Flux chain:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"rules = [\n    ZBoxRule(),\n    GammaRule(),\n    GammaRule(),\n    EpsilonRule(),\n    EpsilonRule(),\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\n\nanalyzer = LRP(model, rules)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Since some Flux Chains contain other Flux Chains, ExplainableAI provides a utility function called flatten_model.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"warning: Flattening models\nNot all models can be flattened, e.g. those using Parallel and SkipConnection layers.","category":"page"},{"location":"generated/advanced_lrp/#Custom-LRP-rules","page":"Advanced LRP","title":"Custom LRP rules","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Let's define a rule that modifies the weights and biases of our layer on the forward pass. The rule has to be of type AbstractLRPRule.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"struct MyGammaRule <: AbstractLRPRule end","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"It is then possible to dispatch on the utility functions  modify_params and modify_denominator with the rule type MyCustomLRPRule to define custom rules without writing any boilerplate code. To extend internal functions, import them explicitly:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"import ExplainableAI: modify_params\n\nfunction modify_params(::MyGammaRule, W, b)\n    ρW = W + 0.25 * relu.(W)\n    ρb = b + 0.25 * relu.(b)\n    return ρW, ρb\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"We can directly use this rule to make an analyzer!","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"analyzer = LRP(model, MyGammaRule())\nheatmap(input, analyzer)","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"We just implemented our own version of the γ-rule in 7 lines of code! The outputs match perfectly:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"analyzer = LRP(model, GammaRule())\nheatmap(input, analyzer)","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"If the layer doesn't use weights and biases W and b, ExplainableAI provides a lower-level variant of modify_params called modify_layer. This function is expected to take a layer and return a new, modified layer.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"warning: Using modify_layer\nUse of the function modify_layer will overwrite functionality of modify_params for the implemented combination of rule and layer types. This is due to the fact that internally, modify_params is called by the default implementation of modify_layer.Therefore it is recommended to only extend modify_layer for a specific rule and a specific layer type.","category":"page"},{"location":"generated/advanced_lrp/#Custom-layers-and-activation-functions","page":"Advanced LRP","title":"Custom layers and activation functions","text":"","category":"section"},{"location":"generated/advanced_lrp/#Model-checks-for-humans","page":"Advanced LRP","title":"Model checks for humans","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Good model checks and presets should allow novice users to apply XAI methods in a \"plug & play\" manner according to best practices.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Let's say we define a layer that doubles its input:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"struct MyDoublingLayer end\n(::MyDoublingLayer)(x) = 2 * x\n\nmylayer = MyDoublingLayer()\nmylayer([1, 2, 3])","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Let's append this layer to our model:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"model = Chain(model..., MyDoublingLayer())","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Creating an LRP analyzer, e.g. LRPZero(model), will throw an ArgumentError and print a summary of the model check in the REPL:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"┌───┬───────────────────────┬─────────────────┬────────────┬────────────────┐\n│   │ Layer                 │ Layer supported │ Activation │ Act. supported │\n├───┼───────────────────────┼─────────────────┼────────────┼────────────────┤\n│ 1 │ flatten               │            true │     —      │           true │\n│ 2 │ Dense(784, 100, relu) │            true │    relu    │           true │\n│ 3 │ Dense(100, 10)        │            true │  identity  │           true │\n│ 4 │ MyDoublingLayer()     │           false │     —      │           true │\n└───┴───────────────────────┴─────────────────┴────────────┴────────────────┘\n  Layers failed model check\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found unknown layers MyDoublingLayer() that are not supported by ExplainableAI's LRP implementation yet.\n\n  If you think the missing layer should be supported by default, please submit an issue (https://github.com/adrhill/ExplainableAI.jl/issues).\n\n  These model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument skip_checks=true.\n\n  [...]","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"LRP should only be used on deep rectifier networks and ExplainableAI doesn't recognize MyDoublingLayer as a compatible layer. By default, it will therefore return an error and a model check summary instead of returning an incorrect explanation.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"However, if we know MyDoublingLayer is compatible with deep rectifier networks, we can register it to tell ExplainableAI that it is ok to use. This will be shown in the following section.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"warning: Skipping model checks\nAll model checks can be skipped at the user's own risk by setting the LRP-analyzer keyword argument skip_checks=true.","category":"page"},{"location":"generated/advanced_lrp/#Registering-custom-layers","page":"Advanced LRP","title":"Registering custom layers","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The error in the model check will stop after registering our custom layer type MyDoublingLayer as \"supported\" by ExplainableAI.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"This is done using the function LRP_CONFIG.supports_layer, which should be set to return true for the type MyDoublingLayer:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"LRP_CONFIG.supports_layer(::MyDoublingLayer) = true","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Now we can create and run an analyzer without getting an error:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"analyzer = LRPZero(model)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"note: Registering functions\nFlux's Chains can also contain functions, e.g. flatten. This kind of layer can be registered asLRP_CONFIG.supports_layer(::typeof(mylayer)) = true","category":"page"},{"location":"generated/advanced_lrp/#Registering-activation-functions","page":"Advanced LRP","title":"Registering activation functions","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The mechanism for registering custom activation functions is analogous to that of custom layers:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"myrelu(x) = max.(0, x)\nmodel = Chain(flatten, Dense(784, 100, myrelu), Dense(100, 10))","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Once again, creating an LRP analyzer for this model will throw an ArgumentError and display the following model check summary:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"julia> analyzer = LRPZero(model3)\n┌───┬─────────────────────────┬─────────────────┬────────────┬────────────────┐\n│   │ Layer                   │ Layer supported │ Activation │ Act. supported │\n├───┼─────────────────────────┼─────────────────┼────────────┼────────────────┤\n│ 1 │ flatten                 │            true │     —      │           true │\n│ 2 │ Dense(784, 100, myrelu) │            true │   myrelu   │          false │\n│ 3 │ Dense(100, 10)          │            true │  identity  │           true │\n└───┴─────────────────────────┴─────────────────┴────────────┴────────────────┘\n  Activations failed model check\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found layers with unknown or unsupported activation functions myrelu. LRP assumes that the model is a \"deep rectifier network\" that only contains ReLU-like activation functions.\n\n  If you think the missing activation function should be supported by default, please submit an issue (https://github.com/adrhill/ExplainableAI.jl/issues).\n\n  These model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument skip_checks=true.\n\n  [...]","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Registation works by defining the function LRP_CONFIG.supports_activation as true:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"LRP_CONFIG.supports_activation(::typeof(myrelu)) = true","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"now the analyzer can be created without error:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"analyzer = LRPZero(model)","category":"page"},{"location":"generated/advanced_lrp/#How-it-works-internally","page":"Advanced LRP","title":"How it works internally","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Internally, ExplainableAI dispatches to low level functions","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(rule, layer, Rₖ, aₖ, Rₖ₊₁)\n    Rₖ .= ...\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"These functions use the arguments rule and layer to dispatch modify_params and modify_denominator on the rule and layer type. They in-place modify a pre-allocated array of the input relevance Rₖ based on the input activation aₖ and output relevance Rₖ₊₁.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Calling analyze then applies a foward-pass of the model, keeping track of the activations aₖ for each layer k. The relevance Rₖ₊₁ is then set to the output neuron activation and the rules are applied in a backward-pass over the model layers and previous activations.","category":"page"},{"location":"generated/advanced_lrp/#Generic-rule-implementation-using-automatic-differentiation","page":"Advanced LRP","title":"Generic rule implementation using automatic differentiation","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The generic LRP rule–of which the 0-, epsilon- and gamma-rules are special cases–reads[1][2]:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"R_j=sum_k fraca_j cdot rholeft(w_j kright)epsilon+sum_0 j a_j cdot rholeft(w_j kright) R_k","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"where rho is a function that modifies parameters – what we have so far called modify_params.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The computation of this propagation rule can be decomposed into four steps:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"beginarraylr\nforall_k z_k=epsilon+sum_0 j a_j cdot rholeft(w_j kright)  text  (forward pass)  \nforall_k s_k=R_k  z_k  text  (element-wise division)  \nforall_j c_j=sum_k rholeft(w_j kright) cdot s_k  text  (backward pass)  \nforall_j R_j=a_j c_j  text  (element-wise product) \nendarray","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"For deep rectifier networks, the third step can also be written as the gradient computation","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"c_j=leftnablaleft(sum_k z_k(boldsymbola) cdot s_kright)right_j","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"and can be implemented via automatic differentiation (AD).","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"This equation is implemented in ExplainableAI as the default method for all layer types that don't have a specialized implementation. We will refer to it as the \"AD fallback\".","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"[1]: G. Montavon et al., Layer-Wise Relevance Propagation: An Overview","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"[2]: W. Samek et al., Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications","category":"page"},{"location":"generated/advanced_lrp/#AD-fallback","page":"Advanced LRP","title":"AD fallback","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The default LRP fallback for unknown layers uses AD via Zygote. For lrp!, we end up with something that looks very similar to the previous four step computation:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(rule, layer, Rₖ, aₖ, Rₖ₊₁)\n    layerᵨ = modify_layer(rule, layer)\n    c = gradient(aₖ) do a\n            z = layerᵨ(a)\n            s = Zygote.@ignore Rₖ₊₁ ./ modify_denominator(rule, z)\n            z ⋅ s\n\t      end |> only\n    Rₖ .= aₖ .* c\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"You can see how modify_layer and modify_denominator dispatch on the rule and layer type. This is how we implemented our own MyGammaRule. Unknown layers that are registered in the LRP_CONFIG use this exact function.","category":"page"},{"location":"generated/advanced_lrp/#Specialized-implementations","page":"Advanced LRP","title":"Specialized implementations","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"We can also implement specialized versions of lrp! based on the type of layer, e.g. reshaping layers.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Reshaping layers don't affect attributions. We can therefore avoid the computational overhead of AD by writing a specialized implementation that simply reshapes back:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(::AbstractLRPRule, ::ReshapingLayer, Rₖ, aₖ, Rₖ₊₁)\n    Rₖ .= reshape(Rₖ₊₁, size(aₖ))\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Since the rule type didn't matter in this case, we didn't specify it.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"We can even implement the generic rule as a specialized implementation for Dense layers:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(rule::AbstractLRPRule, layer::Dense, Rₖ, aₖ, Rₖ₊₁)\n    ρW, ρb = modify_params(rule, get_params(layer)...)\n    ãₖ₊₁ = modify_denominator(rule, ρW * aₖ + ρb)\n    @tullio Rₖ[j] = aₖ[j] * ρW[k, j] / ãₖ₊₁[k] * Rₖ₊₁[k] # Tullio ≈ fast einsum\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"For maximum low-level control beyond modify_layer, modify_params and modify_denominator, you can also implement your own lrp! function and dispatch on individual rule types MyRule and layer types MyLayer:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(rule::MyRule, layer::MyLayer, Rₖ, aₖ, Rₖ₊₁)\n    Rₖ .= ...\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"EditURL = \"https://github.com/adrhill/ExplainableAI.jl/blob/master/docs/literate/example.jl\"","category":"page"},{"location":"generated/example/#Getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"ExplainableAI.jl can be used on any classifier. In this first example, we will look at attributions on a LeNet5 model that was pretrained on MNIST.","category":"page"},{"location":"generated/example/#Loading-the-model","page":"Getting started","title":"Loading the model","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"note: Note\nOutside of these docs, you should be able to load the model usingusing BSON: @load\n@load \"model.bson\" model","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"using ExplainableAI\nusing Flux\nusing BSON\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model]","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"warning: Strip softmax\nFor models with softmax activations on the output, it is necessary to callmodel = strip_softmax(model)before analyzing.","category":"page"},{"location":"generated/example/#Loading-MNIST","page":"Getting started","title":"Loading MNIST","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"We use MLDatasets to load a single image from the MNIST dataset:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"using MLDatasets\nusing ImageCore\n\nindex = 10\nx, y = MNIST.testdata(Float32, index)\n\nMNIST.convert2image(x)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"By convention in Flux.jl, this input needs to be resized to WHCN format by adding a color channel and batch dimensions.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"input = reshape(x, 28, 28, 1, :);\nnothing #hide","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"warning: Input format\nFor any attribution of a model, ExplainableAI.jl assumes the batch dimension to be come last in the input.For the purpose of heatmapping, the input is assumed to be in WHCN order (width, height, channels, batch), which is Flux.jl's convention.","category":"page"},{"location":"generated/example/#Calling-the-analyzer","page":"Getting started","title":"Calling the analyzer","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"We can now select an analyzer of our choice and call analyze to get an Explanation:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"analyzer = LRP(model)\nexpl = analyze(input, analyzer);\nnothing #hide","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This Explanation bundles the following data:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"expl.attribution: the analyzer's attribution\nexpl.output: the model output\nexpl.neuron_selection: the neuron index of used for the attribution\nexpl.analyzer: a symbol corresponding the used analyzer, e.g. :LRP","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Finally, we can visualize the Explanation through heatmapping:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"heatmap(expl)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Or get the same result by combining both analysis and heatmapping into one step:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"heatmap(input, analyzer)","category":"page"},{"location":"generated/example/#Neuron-selection","page":"Getting started","title":"Neuron selection","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"By passing an additional index to our call to analyze, we can compute the attribution with respect to a specific output neuron. Let's see why the output wasn't interpreted as a 4 (output neuron at index 5)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"heatmap(input, analyzer, 5)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This heatmap shows us that the \"upper loop\" of the hand-drawn 9 has negative relevance with respect to the output neuron corresponding to digit 4!","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"note: Note\nThe ouput neuron can also be specified when calling analyze:expl = analyze(img, analyzer, 5)","category":"page"},{"location":"generated/example/#Input-batches","page":"Getting started","title":"Input batches","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"ExplainableAI also supports input batches:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"batchsize = 100\nxs, _ = MNIST.testdata(Float32, 1:batchsize)\nbatch = reshape(xs, 28, 28, 1, :) # reshape to WHCN format\nexpl_batch = analyze(batch, analyzer);\nnothing #hide","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This will once again return a single Explanation expl_batch for the entire batch. Calling heatmap on expl_batch will detect the batch dimension and return a vector of heatmaps.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Let's check if the digit at index = 10 still matches.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"hs = heatmap(expl_batch)\nhs[index]","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"JuliaImages' mosaic can be used to return a tiled view of all heatmaps:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"mosaic(hs; nrow=10)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"We can also evaluate a batch with respect to a specific output neuron, e.g. for the digit zero at index 1:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"mosaic(heatmap(batch, analyzer, 1); nrow=10)","category":"page"},{"location":"generated/example/#Automatic-heatmap-presets","page":"Getting started","title":"Automatic heatmap presets","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Currently, the following analyzers are implemented:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"├── Gradient\n├── InputTimesGradient\n├── SmoothGrad\n└── LRP\n    ├── LRPZero\n    ├── LRPEpsilon\n    └── LRPGamma","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Let's try InputTimesGradient","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"analyzer = InputTimesGradient(model)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"and Gradient","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"analyzer = Gradient(model)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"As you can see, the function heatmap automatically applies common presets for each method.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Since InputTimesGradient and LRP both compute attributions, their presets are similar. Gradient methods however are typically shown in grayscale.","category":"page"},{"location":"generated/example/#Custom-heatmap-settings","page":"Getting started","title":"Custom heatmap settings","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"We can partially or fully override presets by passing keyword arguments to heatmap:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"using ColorSchemes\nheatmap(expl; cs=ColorSchemes.jet)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"heatmap(expl; reduce=:sum, normalize=:extrema, cs=ColorSchemes.inferno)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This also works with batches","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"mosaic(heatmap(expl_batch; normalize=:extrema, cs=ColorSchemes.inferno); nrow=10)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"For the full list of keyword arguments, refer to the heatmap documentation.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ExplainableAI","category":"page"},{"location":"#ExplainableAI.jl","page":"Home","title":"ExplainableAI.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Explainable AI in Julia using Flux.jl.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install this package and its dependencies, open the Julia REPL and run ","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add ExplainableAI","category":"page"},{"location":"#Manual","page":"Home","title":"Manual","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"generated/example.md\",\n    \"generated/advanced_lrp.md\",\n]\nDepth = 2","category":"page"},{"location":"#API-reference","page":"Home","title":"API reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"api.md\",]\nDepth = 2","category":"page"}]
}
