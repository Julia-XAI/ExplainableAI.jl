{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Supporting new layers and activation functions\n",
    "One of the design goals of ExplainableAI.jl is to combine ease of use and\n",
    "extensibility for the purpose of research.\n",
    "This example will show you how to extent LRP to new layer types and activation functions."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using Flux\n",
    "using ExplainableAI"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model checks\n",
    "To assure that novice users use LRP according to best practices,\n",
    "ExplainableAI.jl runs strict model checks when creating an `LRP` analyzer.\n",
    "\n",
    "Let's demonstrate this by defining a new layer type that doubles its input"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3-element Vector{Int64}:\n 2\n 4\n 6"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "struct MyDoublingLayer end\n",
    "(::MyDoublingLayer)(x) = 2 * x\n",
    "\n",
    "mylayer = MyDoublingLayer()\n",
    "mylayer([1, 2, 3])"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "and by defining a model that uses this layer:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "model = Chain(\n",
    "    Dense(100, 20),\n",
    "    MyDoublingLayer()\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating an LRP analyzer, e.g. `LRP(model)`, will throw an `ArgumentError`\n",
    "and print a summary of the model check in the REPL:\n",
    "\n",
    "```julia-repl\n",
    "julia> LRP(model)\n",
    "  ChainTuple(\n",
    "    Dense(100 => 20)  => supported,\n",
    "    MyDoublingLayer() => unknown layer type,\n",
    "  ),\n",
    "\n",
    "  LRP model check failed\n",
    "  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n",
    "\n",
    "  Found unknown layer types or activation functions that are not supported by ExplainableAI's LRP implementation yet.\n",
    "\n",
    "  LRP assumes that the model is a deep rectifier network that only contains ReLU-like activation functions.\n",
    "\n",
    "  If you think the missing layer should be supported by default, please submit an issue (https://github.com/adrhill/ExplainableAI.jl/issues).\n",
    "\n",
    "  [...]\n",
    "\n",
    "ERROR: Unknown layer or activation function found in model\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "LRP should only be used on deep rectifier networks and ExplainableAI doesn't\n",
    "recognize `MyDoublingLayer` as a compatible layer by default.\n",
    "It will therefore return an error and a model check summary\n",
    "instead of returning an incorrect explanation.\n",
    "\n",
    "However, if we know `MyDoublingLayer` is compatible with deep rectifier networks,\n",
    "we can register it to tell ExplainableAI that it is ok to use.\n",
    "This will be shown in the following section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Registering layers\n",
    "The error in the model check will stop after registering our custom layer type\n",
    "`MyDoublingLayer` as \"supported\" by ExplainableAI.\n",
    "\n",
    "This is done using the function `LRP_CONFIG.supports_layer`,\n",
    "which should be set to return `true` for the type `MyDoublingLayer`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "LRP_CONFIG.supports_layer(::MyDoublingLayer) = true"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can create and run an analyzer without getting an error:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Dense(100 => 20)                 \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Main.var\"##338\".MyDoublingLayer()\u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "analyzer = LRP(model)"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Registering activation functions\n",
    "The mechanism for registering custom activation functions is analogous to that of custom layers:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "myrelu(x) = max.(0, x)\n",
    "\n",
    "model = Chain(\n",
    "    Dense(784, 100, myrelu),\n",
    "    Dense(100, 10),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once again, creating an LRP analyzer for this model will throw an `ArgumentError`\n",
    "and display the following model check summary:\n",
    "\n",
    "```julia-repl\n",
    "julia> LRP(model)\n",
    "  ChainTuple(\n",
    "    Dense(784 => 100, myrelu) => unsupported or unknown activation function myrelu,\n",
    "    Dense(100 => 10)          => supported,\n",
    "  ),\n",
    "\n",
    "  LRP model check failed\n",
    "  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n",
    "\n",
    "  Found unknown layer types or activation functions that are not supported by ExplainableAI's LRP implementation yet.\n",
    "\n",
    "  LRP assumes that the model is a deep rectifier network that only contains ReLU-like activation functions.\n",
    "\n",
    "  If you think the missing layer should be supported by default, please submit an issue (https://github.com/adrhill/ExplainableAI.jl/issues).\n",
    "\n",
    "  [...]\n",
    "\n",
    "ERROR: Unknown layer or activation function found in model\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Registation works by defining the function `LRP_CONFIG.supports_activation` as `true`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "LRP_CONFIG.supports_activation(::typeof(myrelu)) = true"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "now the analyzer can be created without error:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Dense(784 => 100, myrelu)\u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dense(100 => 10)         \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "analyzer = LRP(model)"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Skipping model checks\n",
    "All model checks can be skipped at your own risk by setting the LRP-analyzer\n",
    "keyword argument `skip_checks=true`."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Dense(100 => 20, unknown_activation)\u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Main.var\"##338\".MyDoublingLayer()   \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "struct UnknownLayer end\n",
    "(::UnknownLayer)(x) = x\n",
    "\n",
    "unknown_activation(x) = max.(0, x)\n",
    "\n",
    "model = Chain(Dense(100, 20, unknown_activation), MyDoublingLayer())\n",
    "\n",
    "LRP(model; skip_checks=true)"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instead of throwing the usual `ERROR: Unknown layer or activation function found in model`,\n",
    "the LRP analyzer was created without having to register either the layer `UnknownLayer`\n",
    "or the activation function `unknown_activation`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
