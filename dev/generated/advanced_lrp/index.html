<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Advanced LRP · ExplainableAI.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="ExplainableAI.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ExplainableAI.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../example/">Getting started</a></li><li class="is-active"><a class="tocitem" href>Advanced LRP</a><ul class="internal"><li><a class="tocitem" href="#LRP-composites"><span>LRP composites</span></a></li><li><a class="tocitem" href="#Custom-LRP-rules"><span>Custom LRP rules</span></a></li><li><a class="tocitem" href="#Custom-layers-and-activation-functions"><span>Custom layers and activation functions</span></a></li><li><a class="tocitem" href="#How-it-works-internally"><span>How it works internally</span></a></li></ul></li><li><a class="tocitem" href="../../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Advanced LRP</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Advanced LRP</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/adrhill/ExplainableAI.jl/blob/master/docs/literate/advanced_lrp.jl" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Advanced-LRP-usage"><a class="docs-heading-anchor" href="#Advanced-LRP-usage">Advanced LRP usage</a><a id="Advanced-LRP-usage-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-LRP-usage" title="Permalink"></a></h1><p>One of the design goals of ExplainableAI.jl is to combine ease of use and extensibility for the purpose of research.</p><p>This example will show you how to implement custom LRP rules and register custom layers and activation functions. For this purpose, we will quickly load our model from the previous section</p><pre><code class="language-julia hljs">using ExplainableAI
using Flux
using MLDatasets
using ImageCore
using BSON

model = BSON.load(&quot;../model.bson&quot;, @__MODULE__)[:model]</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
  Conv((5, 5), 1 =&gt; 6, relu),           # 156 parameters
  MaxPool((2, 2)),
  Conv((5, 5), 6 =&gt; 16, relu),          # 2_416 parameters
  MaxPool((2, 2)),
  Flux.flatten,
  Dense(256 =&gt; 120, relu),              # 30_840 parameters
  Dense(120 =&gt; 84, relu),               # 10_164 parameters
  Dense(84 =&gt; 10),                      # 850 parameters
)                   # Total: 10 arrays, 44_426 parameters, 174.867 KiB.</code></pre><p>and data from the MNIST dataset</p><pre><code class="language-julia hljs">index = 10
x, _ = MNIST(Float32, :test)[10]
input = reshape(x, 28, 28, 1, :);</code></pre><h2 id="LRP-composites"><a class="docs-heading-anchor" href="#LRP-composites">LRP composites</a><a id="LRP-composites-1"></a><a class="docs-heading-anchor-permalink" href="#LRP-composites" title="Permalink"></a></h2><h3 id="Assigning-individual-rules"><a class="docs-heading-anchor" href="#Assigning-individual-rules">Assigning individual rules</a><a id="Assigning-individual-rules-1"></a><a class="docs-heading-anchor-permalink" href="#Assigning-individual-rules" title="Permalink"></a></h3><p>When creating an LRP-analyzer, we can assign individual rules to each layer. The array of rules has to match the length of the Flux chain. The <code>LRP</code> analyzer will show a summary of how layers and rules got matched:</p><pre><code class="language-julia hljs">rules = [
    ZBoxRule(0.0f0, 1.0f0),
    EpsilonRule(),
    GammaRule(),
    EpsilonRule(),
    ZeroRule(),
    ZeroRule(),
    ZeroRule(),
    ZeroRule(),
]

analyzer = LRP(model, rules)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">LRP(
  Conv((5, 5), 1 =&gt; 6, relu)   =&gt; ZBoxRule{Float32}(0.0f0, 1.0f0),
  MaxPool((2, 2))              =&gt; EpsilonRule{Float32}(1.0f-6),
  Conv((5, 5), 6 =&gt; 16, relu)  =&gt; GammaRule{Float32}(0.25f0),
  MaxPool((2, 2))              =&gt; EpsilonRule{Float32}(1.0f-6),
  Flux.flatten                 =&gt; ZeroRule(),
  Dense(256 =&gt; 120, relu)      =&gt; ZeroRule(),
  Dense(120 =&gt; 84, relu)       =&gt; ZeroRule(),
  Dense(84 =&gt; 10)              =&gt; ZeroRule(),
)
</code></pre><pre><code class="language-julia hljs">heatmap(input, analyzer)</code></pre><img src="data:image/png;base64,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"><p>Since some Flux Chains contain other Flux Chains, ExplainableAI provides a utility function called <a href="../../api/#ExplainableAI.flatten_model"><code>flatten_model</code></a>.</p><div class="admonition is-warning"><header class="admonition-header">Flattening models</header><div class="admonition-body"><p>Not all models can be flattened, e.g. those using <code>Parallel</code> and <code>SkipConnection</code> layers.</p></div></div><h3 id="Custom-composites"><a class="docs-heading-anchor" href="#Custom-composites">Custom composites</a><a id="Custom-composites-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-composites" title="Permalink"></a></h3><p>Instead of manually defining a list of rules, we can also use a <a href="../../api/#ExplainableAI.Composite"><code>Composite</code></a>. A composite contructs a list of LRP-rules by sequentially applying <a href="../../api/#composite_primitive_api">Composite primitives</a> it contains.</p><p>To obtain the same set of rules as in the previous example, we can define</p><pre><code class="language-julia hljs">composite = Composite(
    ZeroRule(),                              # default rule
    GlobalTypeRule(
        Conv =&gt; GammaRule(),                 # apply GammaRule on all convolutional layers
        MaxPool =&gt; EpsilonRule(),            # apply EpsilonRule on all pooling-layers
    ),
    FirstLayerRule(ZBoxRule(0.0f0, 1.0f0)),  # apply ZBoxRule on the first layer
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Composite(
  GlobalRule: all layers =&gt; ZeroRule(),
  GlobalTypeRule(     # on all layers
    Flux.Conv =&gt; GammaRule{Float32}(0.25f0),
    Flux.MaxPool =&gt; EpsilonRule{Float32}(1.0f-6),
  ),
  FirstLayerRule: first layer =&gt; ZBoxRule{Float32}(0.0f0, 1.0f0),
)
</code></pre><p>We now construct an LRP analyzer from <code>composite</code></p><pre><code class="language-julia hljs">analyzer = LRP(model, composite)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">LRP(
  Conv((5, 5), 1 =&gt; 6, relu)   =&gt; ZBoxRule{Float32}(0.0f0, 1.0f0),
  MaxPool((2, 2))              =&gt; EpsilonRule{Float32}(1.0f-6),
  Conv((5, 5), 6 =&gt; 16, relu)  =&gt; GammaRule{Float32}(0.25f0),
  MaxPool((2, 2))              =&gt; EpsilonRule{Float32}(1.0f-6),
  Flux.flatten                 =&gt; ZeroRule(),
  Dense(256 =&gt; 120, relu)      =&gt; ZeroRule(),
  Dense(120 =&gt; 84, relu)       =&gt; ZeroRule(),
  Dense(84 =&gt; 10)              =&gt; ZeroRule(),
)
</code></pre><p>As you can see, this analyzer contains the same rules as our previous one and therefore also produces the same heatmaps:</p><pre><code class="language-julia hljs">heatmap(input, analyzer)</code></pre><img src="data:image/png;base64,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"><h3 id="Composite-primitives"><a class="docs-heading-anchor" href="#Composite-primitives">Composite primitives</a><a id="Composite-primitives-1"></a><a class="docs-heading-anchor-permalink" href="#Composite-primitives" title="Permalink"></a></h3><p>The following <a href="../../api/#composite_primitive_api">Composite primitives</a> can used to construct a <a href="../../api/#ExplainableAI.Composite"><code>Composite</code></a>.</p><p>To apply a single rule, use:</p><ul><li><a href="../../api/#ExplainableAI.LayerRule"><code>LayerRule</code></a> to apply a rule to the <code>n</code>-th layer of a model</li><li><a href="../../api/#ExplainableAI.GlobalRule"><code>GlobalRule</code></a> to apply a rule to all layers</li><li><a href="../../api/#ExplainableAI.RangeRule"><code>RangeRule</code></a> to apply a rule to a positional range of layers</li><li><a href="../../api/#ExplainableAI.FirstLayerRule"><code>FirstLayerRule</code></a> to apply a rule to the first layer</li><li><a href="../../api/#ExplainableAI.LastLayerRule"><code>LastLayerRule</code></a> to apply a rule to the last layer</li></ul><p>To apply a set of rules to layers based on their type, use:</p><ul><li><a href="../../api/#ExplainableAI.GlobalTypeRule"><code>GlobalTypeRule</code></a> to apply a dictionary that maps layer types to LRP-rules</li><li><a href="../../api/#ExplainableAI.RangeTypeRule"><code>RangeTypeRule</code></a> for a <code>TypeRule</code> on generalized ranges</li><li><a href="../../api/#ExplainableAI.FirstLayerTypeRule"><code>FirstLayerTypeRule</code></a> for a <code>TypeRule</code> on the first layer of a model</li><li><a href="../../api/#ExplainableAI.LastLayerTypeRule"><code>LastLayerTypeRule</code></a> for a <code>TypeRule</code> on the last layer</li><li><a href="../../api/#ExplainableAI.FirstNTypeRule"><code>FirstNTypeRule</code></a> for a <code>TypeRule</code> on the first <code>n</code> layers</li><li><a href="../../api/#ExplainableAI.LastNTypeRule"><code>LastNTypeRule</code></a> for a <code>TypeRule</code> on the last <code>n</code> layers</li></ul><p>Primitives are called sequentially in the order the <code>Composite</code> was created with and overwrite rules specified by previous primitives.</p><h3 id="Default-composites"><a class="docs-heading-anchor" href="#Default-composites">Default composites</a><a id="Default-composites-1"></a><a class="docs-heading-anchor-permalink" href="#Default-composites" title="Permalink"></a></h3><p>A list of implemented default composites can be found under <a href="../../api/#default_composite_api">Default composites</a> in the API reference, e.g. <a href="../../api/#ExplainableAI.EpsilonPlusFlat"><code>EpsilonPlusFlat</code></a>:</p><pre><code class="language-julia hljs">composite = EpsilonPlusFlat()</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Composite(
  GlobalTypeRule(     # on all layers
    Union{Flux.Conv, Flux.ConvTranspose, Flux.CrossCor} =&gt; ZPlusRule(),
    Flux.Dense =&gt; EpsilonRule{Float32}(1.0f-6),
    Union{typeof(Flux.dropout), Flux.AlphaDropout, Flux.Dropout} =&gt; PassRule(),
    Union{typeof(Flux.flatten), typeof(MLUtils.flatten)} =&gt; PassRule(),
  ),
  FirstLayerTypeRule( # on first layer
    Union{Flux.Conv, Flux.ConvTranspose, Flux.CrossCor} =&gt; FlatRule(),
    Flux.Dense =&gt; FlatRule(),
  ),
)
</code></pre><pre><code class="language-julia hljs">analyzer = LRP(model, composite)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">LRP(
  Conv((5, 5), 1 =&gt; 6, relu)   =&gt; FlatRule(),
  MaxPool((2, 2))              =&gt; ZeroRule(),
  Conv((5, 5), 6 =&gt; 16, relu)  =&gt; ZPlusRule(),
  MaxPool((2, 2))              =&gt; ZeroRule(),
  Flux.flatten                 =&gt; PassRule(),
  Dense(256 =&gt; 120, relu)      =&gt; EpsilonRule{Float32}(1.0f-6),
  Dense(120 =&gt; 84, relu)       =&gt; EpsilonRule{Float32}(1.0f-6),
  Dense(84 =&gt; 10)              =&gt; EpsilonRule{Float32}(1.0f-6),
)
</code></pre><h2 id="Custom-LRP-rules"><a class="docs-heading-anchor" href="#Custom-LRP-rules">Custom LRP rules</a><a id="Custom-LRP-rules-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-LRP-rules" title="Permalink"></a></h2><p>Let&#39;s define a rule that modifies the weights and biases of our layer on the forward pass. The rule has to be of type <code>AbstractLRPRule</code>.</p><pre><code class="language-julia hljs">struct MyGammaRule &lt;: AbstractLRPRule end</code></pre><p>It is then possible to dispatch on the utility functions <a href="../../api/#ExplainableAI.modify_input"><code>modify_input</code></a>, <a href="../../api/#ExplainableAI.modify_param!"><code>modify_param!</code></a> and <a href="../../api/#ExplainableAI.modify_denominator"><code>modify_denominator</code></a> with the rule type <code>MyCustomLRPRule</code> to define custom rules without writing any boilerplate code. To extend internal functions, import them explicitly:</p><pre><code class="language-julia hljs">import ExplainableAI: modify_param!

function modify_param!(::MyGammaRule, param)
    param .+= 0.25f0 * relu.(param)
    return nothing
end</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">modify_param! (generic function with 7 methods)</code></pre><p>We can directly use this rule to make an analyzer!</p><pre><code class="language-julia hljs">rules = [
    ZBoxRule(0.0f0, 1.0f0),
    EpsilonRule(),
    MyGammaRule(),
    EpsilonRule(),
    ZeroRule(),
    ZeroRule(),
    ZeroRule(),
    ZeroRule(),
]
analyzer = LRP(model, rules)
heatmap(input, analyzer)</code></pre><img src="data:image/png;base64,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"><p>We just implemented our own version of the <span>$γ$</span>-rule in 4 lines of code. The heatmap perfectly matches the previous one!</p><p>If the layer doesn&#39;t use weights <code>layer.weight</code> and biases <code>layer.bias</code>, ExplainableAI provides a lower-level variant of <a href="../../api/#ExplainableAI.modify_param!"><code>modify_param!</code></a> called <a href="../../api/#ExplainableAI.modify_layer!"><code>modify_layer!</code></a>. This function is expected to take a layer and return a new, modified layer. To add compatibility checks between rule and layer types, extend <a href="../../api/#ExplainableAI.check_compat"><code>check_compat</code></a>.</p><div class="admonition is-warning"><header class="admonition-header">Using modify_layer!</header><div class="admonition-body"><p>Use of the function <code>modify_layer!</code> will overwrite functionality of <code>modify_param!</code> for the implemented combination of rule and layer types. This is due to the fact that internally, <code>modify_param!</code> is called by the default implementation of <code>modify_layer!</code>.</p><p>Therefore it is recommended to only extend <code>modify_layer!</code> for a specific rule and a specific layer type.</p></div></div><h2 id="Custom-layers-and-activation-functions"><a class="docs-heading-anchor" href="#Custom-layers-and-activation-functions">Custom layers and activation functions</a><a id="Custom-layers-and-activation-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Custom-layers-and-activation-functions" title="Permalink"></a></h2><h3 id="Model-checks-for-humans"><a class="docs-heading-anchor" href="#Model-checks-for-humans">Model checks for humans</a><a id="Model-checks-for-humans-1"></a><a class="docs-heading-anchor-permalink" href="#Model-checks-for-humans" title="Permalink"></a></h3><p>Good model checks and presets should allow novice users to apply XAI methods in a &quot;plug &amp; play&quot; manner according to best practices.</p><p>Let&#39;s say we define a layer that doubles its input:</p><pre><code class="language-julia hljs">struct MyDoublingLayer end
(::MyDoublingLayer)(x) = 2 * x

mylayer = MyDoublingLayer()
mylayer([1, 2, 3])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">3-element Vector{Int64}:
 2
 4
 6</code></pre><p>Let&#39;s append this layer to our model:</p><pre><code class="language-julia hljs">model = Chain(model..., MyDoublingLayer())</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
  Conv((5, 5), 1 =&gt; 6, relu),           # 156 parameters
  MaxPool((2, 2)),
  Conv((5, 5), 6 =&gt; 16, relu),          # 2_416 parameters
  MaxPool((2, 2)),
  Flux.flatten,
  Dense(256 =&gt; 120, relu),              # 30_840 parameters
  Dense(120 =&gt; 84, relu),               # 10_164 parameters
  Dense(84 =&gt; 10),                      # 850 parameters
  Main.MyDoublingLayer(),
)                   # Total: 10 arrays, 44_426 parameters, 174.867 KiB.</code></pre><p>Creating an LRP analyzer, e.g. <code>LRP(model)</code>, will throw an <code>ArgumentError</code> and print a summary of the model check in the REPL:</p><pre><code class="language-julia-repl hljs">┌───┬───────────────────────┬─────────────────┬────────────┬────────────────┐
│   │ Layer                 │ Layer supported │ Activation │ Act. supported │
├───┼───────────────────────┼─────────────────┼────────────┼────────────────┤
│ 1 │ flatten               │            true │     —      │           true │
│ 2 │ Dense(784, 100, relu) │            true │    relu    │           true │
│ 3 │ Dense(100, 10)        │            true │  identity  │           true │
│ 4 │ MyDoublingLayer()     │           false │     —      │           true │
└───┴───────────────────────┴─────────────────┴────────────┴────────────────┘
  Layers failed model check
  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡

  Found unknown layers MyDoublingLayer() that are not supported by ExplainableAI&#39;s LRP implementation yet.

  If you think the missing layer should be supported by default, please submit an issue (https://github.com/adrhill/ExplainableAI.jl/issues).

  These model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument skip_checks=true.

  [...]</code></pre><p>LRP should only be used on deep rectifier networks and ExplainableAI doesn&#39;t recognize <code>MyDoublingLayer</code> as a compatible layer. By default, it will therefore return an error and a model check summary instead of returning an incorrect explanation.</p><p>However, if we know <code>MyDoublingLayer</code> is compatible with deep rectifier networks, we can register it to tell ExplainableAI that it is ok to use. This will be shown in the following section.</p><div class="admonition is-warning"><header class="admonition-header">Skipping model checks</header><div class="admonition-body"><p>All model checks can be skipped at the user&#39;s own risk by setting the LRP-analyzer keyword argument <code>skip_checks=true</code>.</p></div></div><h3 id="Registering-custom-layers"><a class="docs-heading-anchor" href="#Registering-custom-layers">Registering custom layers</a><a id="Registering-custom-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Registering-custom-layers" title="Permalink"></a></h3><p>The error in the model check will stop after registering our custom layer type <code>MyDoublingLayer</code> as &quot;supported&quot; by ExplainableAI.</p><p>This is done using the function <a href="../../api/#ExplainableAI.LRP_CONFIG.supports_layer"><code>LRP_CONFIG.supports_layer</code></a>, which should be set to return <code>true</code> for the type <code>MyDoublingLayer</code>:</p><pre><code class="language-julia hljs">LRP_CONFIG.supports_layer(::MyDoublingLayer) = true</code></pre><p>Now we can create and run an analyzer without getting an error:</p><pre><code class="language-julia hljs">analyzer = LRP(model)
heatmap(input, analyzer)</code></pre><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAIAAABJgmMcAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAABBtJREFUeAHtwU9I1gcYwPGvvk9qv/fVF9KyDpmZoisZlTUK+kMdtkUdIiiD4bGOa4PYYQ62S4Ug/bl06RCd2lrQcK5asGgs6OCCRRtGWYeNDmVbLNNme/Xd6XnsIETy+Po6ns9H8vk8wY8QXAnBlRBcCcGVEFwJwZUQXAnBlRBcCcGVEFwJwZUQXAnBlRBcCcGVEFwJwZUQXAnBlRBcCcGVEFwJwZUQXAnBlRBcCUUsl2NS0vsN5swZTEMDprMT9U8yD1VRwbQSgishuBKCK6EYDA9jRkZQcu8e5tEjzOAgJpXClJZiBgZQFbW1qO9+XYLKZjHr12NSKaZMCK6E4EoIroRCev4cNZTPoO7eTaPaWpjw9CmmtRVz6xZm3z5Mfz/m5EnMqlWoxvcPoOrqMIODmIULmTIhuBKCKyG4EqbZ+Djm+58yqIcPMbdvY+o+S6MyW7ej5paPY5qamFRtLWr8w49Qw8OY5h97MTeeoOaWl2P27MGkUrwJIbgSgishuBK85POYBw9Qv/y9DLVzJ+blS8zly5hsFlNWxitKeZ3DF1ei9lZhkgRTuWED5vp1zKZNmHyeqRKCKyG4EoIrwUlurAT1w8AyVDaLOXIEs3YtZuNG3sj4OGZoCPPpJznMjRuYJEGdu9+GWr58B6q14iUmlWKqhOBKCK6E4EpwIi+GUO/98RXm5Leot3p6UNlLlzDtp1Gj586hyvv6MKdOoUoPHEBlDx3CNDRgVq7EbN6M2lKH6e3FLF5chsqWMWVCcCUEV0JwJXiprMRUVqJGenpQx3nFtm2oUiZkmPBxVxfq6/PnUbuHhjD372NGRzFVVZjmZtT8pXNRHR1plAguhOBKCK6E4EqYDu3tqGT7dtTnu3ZhVqxA/Xv8OGpOYyMmnUbtbmtDjZw9i0q6uzHPnmFyOUxZGSZJUFKCOyG4EoIrIbgSplsmg7lyhcnMOXaM1zpxAvX7zZuolqtXMQ0NmIMHMUuWUChCcCUEV0JwJRSzsTFMTQ2qZfVqTDqN6erCJAkzQQiuhOBKCK6EItPZiTl8+DdU/vG7qJ+bP0CtWYPJ5zElzAwhuBKCKyG4EopAby/m6FHMli1vo7a2Yx4/xly4gGlqYsYJwZUQXAnBlVAEBgcxS5di5s3DtLZiduzANDVRVITgSgiuhOBKmCEvXmCuXcP09w+jqqrSqO5uTH09RUsIroTgSgiuhBnS14fZvx8zOppGrVuHqa9nVhCCKyG4EoIroZAGBlCbKv5CXXzyDurLL+6g/pzfwmwjBFdCcCUEV0IhnT6N6ehAralnQmkNqrqaWUcIroTgSgiuhEJatAizYAEmx4TqamYzIbgSgishuBIKae9ezNgYKkmYUFLCbCYEV0JwJQRXQiHV1DCZDP8fQnAlBFdCcPUfZaDIbeMVzvAAAAAASUVORK5C"><div class="admonition is-info"><header class="admonition-header">Registering functions</header><div class="admonition-body"><p>Flux&#39;s <code>Chains</code> can also contain functions, e.g. <code>flatten</code>. This kind of layer can be registered as</p><pre><code class="language-julia hljs">LRP_CONFIG.supports_layer(::typeof(mylayer)) = true</code></pre></div></div><h3 id="Registering-activation-functions"><a class="docs-heading-anchor" href="#Registering-activation-functions">Registering activation functions</a><a id="Registering-activation-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Registering-activation-functions" title="Permalink"></a></h3><p>The mechanism for registering custom activation functions is analogous to that of custom layers:</p><pre><code class="language-julia hljs">myrelu(x) = max.(0, x)
model = Chain(Flux.flatten, Dense(784, 100, myrelu), Dense(100, 10))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Chain(
  Flux.flatten,
  Dense(784 =&gt; 100, myrelu),            # 78_500 parameters
  Dense(100 =&gt; 10),                     # 1_010 parameters
)                   # Total: 4 arrays, 79_510 parameters, 310.836 KiB.</code></pre><p>Once again, creating an LRP analyzer for this model will throw an <code>ArgumentError</code> and display the following model check summary:</p><pre><code class="language-julia-repl hljs">julia&gt; analyzer = LRP(model3)
┌───┬─────────────────────────┬─────────────────┬────────────┬────────────────┐
│   │ Layer                   │ Layer supported │ Activation │ Act. supported │
├───┼─────────────────────────┼─────────────────┼────────────┼────────────────┤
│ 1 │ flatten                 │            true │     —      │           true │
│ 2 │ Dense(784, 100, myrelu) │            true │   myrelu   │          false │
│ 3 │ Dense(100, 10)          │            true │  identity  │           true │
└───┴─────────────────────────┴─────────────────┴────────────┴────────────────┘
  Activations failed model check
  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡

  Found layers with unknown or unsupported activation functions myrelu. LRP assumes that the model is a &quot;deep rectifier network&quot; that only contains ReLU-like activation functions.

  If you think the missing activation function should be supported by default, please submit an issue (https://github.com/adrhill/ExplainableAI.jl/issues).

  These model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument skip_checks=true.

  [...]</code></pre><p>Registation works by defining the function <a href="../../api/#ExplainableAI.LRP_CONFIG.supports_activation"><code>LRP_CONFIG.supports_activation</code></a> as <code>true</code>:</p><pre><code class="language-julia hljs">LRP_CONFIG.supports_activation(::typeof(myrelu)) = true</code></pre><p>now the analyzer can be created without error:</p><pre><code class="language-julia hljs">analyzer = LRP(model)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">LRP(
  Flux.flatten               =&gt; ZeroRule(),
  Dense(784 =&gt; 100, myrelu)  =&gt; ZeroRule(),
  Dense(100 =&gt; 10)           =&gt; ZeroRule(),
)
</code></pre><h2 id="How-it-works-internally"><a class="docs-heading-anchor" href="#How-it-works-internally">How it works internally</a><a id="How-it-works-internally-1"></a><a class="docs-heading-anchor-permalink" href="#How-it-works-internally" title="Permalink"></a></h2><p>Internally, ExplainableAI dispatches to low level functions</p><pre><code class="language-julia hljs">lrp!(Rₖ, rule, layer, aₖ, Rₖ₊₁)
    Rₖ .= ...
end</code></pre><p>These functions in-place modify a pre-allocated array of the input relevance <code>Rₖ</code> (the <code>!</code> is a <a href="https://docs.julialang.org/en/v1/manual/style-guide/#bang-convention">naming convention</a> in Julia to denote functions that modify their arguments).</p><p>The correct rule is applied via <a href="https://www.youtube.com/watch?v=kc9HwsxE1OY">multiple dispatch</a> on the types of the arguments <code>rule</code> and <code>layer</code>. The relevance <code>Rₖ</code> is then computed based on the input activation <code>aₖ</code> and the output relevance <code>Rₖ₊₁</code>. Multiple dispatch is also used to dispatch <code>modify_param!</code> and <code>modify_denominator</code> on the rule and layer type.</p><p>Calling <code>analyze</code> on a LRP-model applies a forward-pass of the model, keeping track of the activations <code>aₖ</code> for each layer <code>k</code>. The relevance <code>Rₖ₊₁</code> is then set to the output neuron activation and the rules are applied in a backward-pass over the model layers and previous activations.</p><h3 id="Generic-rule-implementation-using-automatic-differentiation"><a class="docs-heading-anchor" href="#Generic-rule-implementation-using-automatic-differentiation">Generic rule implementation using automatic differentiation</a><a id="Generic-rule-implementation-using-automatic-differentiation-1"></a><a class="docs-heading-anchor-permalink" href="#Generic-rule-implementation-using-automatic-differentiation" title="Permalink"></a></h3><p>The generic LRP rule–of which the <span>$0$</span>-, <span>$\epsilon$</span>- and <span>$\gamma$</span>-rules are special cases–reads<sup class="footnote-reference"><a id="citeref-1" href="#footnote-1">[1]</a></sup><sup class="footnote-reference"><a id="citeref-2" href="#footnote-2">[2]</a></sup>:</p><p class="math-container">\[R_{j}=\sum_{k} \frac{a_{j} \cdot \rho\left(w_{j k}\right)}{\epsilon+\sum_{0, j} a_{j} \cdot \rho\left(w_{j k}\right)} R_{k}\]</p><p>where <span>$\rho$</span> is a function that modifies parameters – what we call <code>modify_param!</code>.</p><p>The computation of this propagation rule can be decomposed into four steps:</p><p class="math-container">\[\begin{array}{lr}
\forall_{k}: z_{k}=\epsilon+\sum_{0, j} a_{j} \cdot \rho\left(w_{j k}\right) &amp; \text { (forward pass) } \\
\forall_{k}: s_{k}=R_{k} / z_{k} &amp; \text { (element-wise division) } \\
\forall_{j}: c_{j}=\sum_{k} \rho\left(w_{j k}\right) \cdot s_{k} &amp; \text { (backward pass) } \\
\forall_{j}: R_{j}=a_{j} c_{j} &amp; \text { (element-wise product) }
\end{array}\]</p><p>For deep rectifier networks, the third step can also be written as the gradient computation</p><p class="math-container">\[c_{j}=\left[\nabla\left(\sum_{k} z_{k}(\boldsymbol{a}) \cdot s_{k}\right)\right]_{j}\]</p><p>and can be implemented via automatic differentiation (AD).</p><p>This equation is implemented in ExplainableAI as the default method for all layer types that don&#39;t have a specialized implementation. We will refer to it as the &quot;AD fallback&quot;.</p><h3 id="AD-fallback"><a class="docs-heading-anchor" href="#AD-fallback">AD fallback</a><a id="AD-fallback-1"></a><a class="docs-heading-anchor-permalink" href="#AD-fallback" title="Permalink"></a></h3><p>The default LRP fallback for unknown layers uses AD via <a href="https://github.com/FluxML/Zygote.jl">Zygote</a>. For <code>lrp!</code>, we implement the previous four step computation using <code>Zygote.pullback</code> to compute <span>$c$</span> from the previous equation as a VJP, pulling back <span>$s_{k}=R_{k}/z_{k}$</span>:</p><pre><code class="language-julia hljs">function lrp!(Rₖ, rule, layer, aₖ, Rₖ₊₁)
   check_compat(rule, layer)
   reset! = get_layer_resetter(layer)
   modify_layer!(rule, layer)
   ãₖ₊₁, pullback = Zygote.pullback(layer, modify_input(rule, aₖ))
   Rₖ .= aₖ .* only(pullback(Rₖ₊₁ ./ modify_denominator(rule, ãₖ₊₁)))
   reset!()
end</code></pre><p>You can see how <code>check_compat</code>, <code>modify_layer!</code>, <code>modify_input</code> and <code>modify_denominator</code> dispatch on the rule and layer type. This is how we implemented our own <code>MyGammaRule</code>. Unknown layers that are registered in the <code>LRP_CONFIG</code> use this exact function.</p><h3 id="Specialized-implementations"><a class="docs-heading-anchor" href="#Specialized-implementations">Specialized implementations</a><a id="Specialized-implementations-1"></a><a class="docs-heading-anchor-permalink" href="#Specialized-implementations" title="Permalink"></a></h3><p>We can also implement specialized versions of <code>lrp!</code> based on the type of <code>layer</code>, e.g. reshaping layers.</p><p>Reshaping layers don&#39;t affect attributions. We can therefore avoid the computational overhead of AD by writing a specialized implementation that simply reshapes back:</p><pre><code class="language-julia hljs">function lrp!(Rₖ, rule, ::ReshapingLayer, aₖ, Rₖ₊₁)
    Rₖ .= reshape(Rₖ₊₁, size(aₖ))
end</code></pre><p>Since the rule type didn&#39;t matter in this case, we didn&#39;t specify it.</p><p>We can even implement the generic rule as a specialized implementation for <code>Dense</code> layers:</p><pre><code class="language-julia hljs">function lrp!(Rₖ, rule, layer::Dense, aₖ, Rₖ₊₁)
    reset! = get_layer_resetter(rule, layer)
    modify_layer!(rule, layer)
    ãₖ₊₁ = modify_denominator(rule, layer(modify_input(rule, aₖ)))
    @tullio Rₖ[j, b] = aₖ[j, b] * layer.weight[k, j] * Rₖ₊₁[k, b] / ãₖ₊₁[k, b] # Tullio ≈ fast einsum
    reset!()
end</code></pre><p>For maximum low-level control beyond <code>modify_layer!</code>, <code>modify_param!</code> and <code>modify_denominator</code>, you can also implement your own <code>lrp!</code> function and dispatch on individual rule types <code>MyRule</code> and layer types <code>MyLayer</code>:</p><pre><code class="language-julia hljs">function lrp!(Rₖ, rule::MyRule, layer::MyLayer, aₖ, Rₖ₊₁)
    Rₖ .= ...
end</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p><section class="footnotes is-size-7"><ul><li class="footnote" id="footnote-1"><a class="tag is-link" href="#citeref-1">1</a>G. Montavon et al., <a href="https://link.springer.com/chapter/10.1007/978-3-030-28954-6_10">Layer-Wise Relevance Propagation: An Overview</a></li><li class="footnote" id="footnote-2"><a class="tag is-link" href="#citeref-2">2</a>W. Samek et al., <a href="https://ieeexplore.ieee.org/document/9369420">Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications</a></li></ul></section></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../example/">« Getting started</a><a class="docs-footer-nextpage" href="../../api/">API Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.22 on <span class="colophon-date" title="Friday 19 August 2022 14:45">Friday 19 August 2022</span>. Using Julia version 1.8.0.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
