var documenterSearchIndex = {"docs":
[{"location":"api/#Basics","page":"API Reference","title":"Basics","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"All methods in ExplainableAI.jl work by calling analyze on an input and an analyzer:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"analyze\nheatmap","category":"page"},{"location":"api/#ExplainableAI.analyze","page":"API Reference","title":"ExplainableAI.analyze","text":"analyze(input, method)\nanalyze(input, method, neuron_selection)\n\nReturn raw classifier output and explanation. If neuron_selection is specified, the explanation will be calculated for that neuron. Otherwise, the output neuron with the highest activation is automatically chosen.\n\nKeyword arguments\n\nadd_batch_dim: add batch dimension to the input without allocating. Default is false.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.heatmap","page":"API Reference","title":"ExplainableAI.heatmap","text":"heatmap(expl::Explanation; kwargs...)\nheatmap(attr::AbstractArray; kwargs...)\n\nheatmap(input, analyzer::AbstractXAIMethod)\nheatmap(input, analyzer::AbstractXAIMethod, neuron_selection::Int)\n\nVisualize explanation. Assumes Flux's WHCN convention (width, height, color channels, batch size).\n\nKeyword arguments\n\ncs::ColorScheme: ColorScheme that is applied.   When calling heatmap with an Explanation or analyzer, the method default is selected.   When calling heatmap with an array, the default is ColorSchemes.bwr.\nreduce::Symbol: How the color channels are reduced to a single number to apply a colorscheme.   The following methods can be selected, which are then applied over the color channels   for each \"pixel\" in the attribution:\n:sum: sum up color channels\n:norm: compute 2-norm over the color channels\n:maxabs: compute maximum(abs, x) over the color channels in\nWhen calling heatmap with an Explanation or analyzer, the method default is selected.   When calling heatmap with an array, the default is :sum.\nrangescale::Symbol: How the color channel reduced heatmap is normalized before the colorscheme is applied.   Can be either :extrema or :centered.   When calling heatmap with an Explanation or analyzer, the method default is selected.   When calling heatmap with an array, the default for use with the bwr colorscheme is :centered.\npermute::Bool: Whether to flip W&H input channels. Default is true.\nunpack_singleton::Bool: When heatmapping a batch with a single sample, setting unpack_singleton=true   will return an image instead of an Vector containing a single image.\n\nNote: these keyword arguments can't be used when calling heatmap with an analyzer.\n\n\n\n\n\n","category":"function"},{"location":"api/#Analyzers","page":"API Reference","title":"Analyzers","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"LRP\nGradient\nInputTimesGradient\nSmoothGrad\nIntegratedGradients","category":"page"},{"location":"api/#ExplainableAI.LRP","page":"API Reference","title":"ExplainableAI.LRP","text":"LRP(c::Chain, r::AbstractLRPRule)\nLRP(c::Chain, rs::AbstractVector{<:AbstractLRPRule})\n\nAnalyze model by applying Layer-Wise Relevance Propagation.\n\nKeyword arguments\n\nskip_checks::Bool: Skip checks whether model is compatible with LRP and contains output softmax. Default is false.\nverbose::Bool: Select whether the model checks should print a summary on failure. Default is true.\n\nReferences\n\n[1] G. Montavon et al., Layer-Wise Relevance Propagation: An Overview [2] W. Samek et al., Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.Gradient","page":"API Reference","title":"ExplainableAI.Gradient","text":"Gradient(model)\n\nAnalyze model by calculating the gradient of a neuron activation with respect to the input.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.InputTimesGradient","page":"API Reference","title":"ExplainableAI.InputTimesGradient","text":"InputTimesGradient(model)\n\nAnalyze model by calculating the gradient of a neuron activation with respect to the input. This gradient is then multiplied element-wise with the input.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.SmoothGrad","page":"API Reference","title":"ExplainableAI.SmoothGrad","text":"SmoothGrad(analyzer, [n=50, std=0.1, rng=GLOBAL_RNG])\nSmoothGrad(analyzer, [n=50, distribution=Normal(0, σ²=0.01), rng=GLOBAL_RNG])\n\nAnalyze model by calculating a smoothed sensitivity map. This is done by averaging sensitivity maps of a Gradient analyzer over random samples in a neighborhood of the input, typically by adding Gaussian noise with mean 0.\n\nReferences\n\n[1] Smilkov et al., SmoothGrad: removing noise by adding noise\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.IntegratedGradients","page":"API Reference","title":"ExplainableAI.IntegratedGradients","text":"IntegratedGradients(analyzer, [n=50])\nIntegratedGradients(analyzer, [n=50])\n\nAnalyze model by using the Integrated Gradients method.\n\nReferences\n\n[1] Sundararajan et al., Axiomatic Attribution for Deep Networks\n\n\n\n\n\n","category":"function"},{"location":"api/","page":"API Reference","title":"API Reference","text":"SmoothGrad and IntegratedGradients are special cases of the input augmentation wrappers NoiseAugmentation and InterpolationAugmentation, which can be applied as a wrapper to any analyzer:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"NoiseAugmentation\nInterpolationAugmentation","category":"page"},{"location":"api/#ExplainableAI.NoiseAugmentation","page":"API Reference","title":"ExplainableAI.NoiseAugmentation","text":"NoiseAugmentation(analyzer, n, [std=1, rng=GLOBAL_RNG])\nNoiseAugmentation(analyzer, n, distribution, [rng=GLOBAL_RNG])\n\nA wrapper around analyzers that augments the input with n samples of additive noise sampled from distribution. This input augmentation is then averaged to return an Explanation.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.InterpolationAugmentation","page":"API Reference","title":"ExplainableAI.InterpolationAugmentation","text":"InterpolationAugmentation(model, [n=50])\n\nA wrapper around analyzers that augments the input with n steps of linear interpolation between the input and a reference input (typically zero(input)). The gradients w.r.t. this augmented input are then averaged and multiplied with the difference between the input and the reference input.\n\n\n\n\n\n","category":"type"},{"location":"api/#LRP","page":"API Reference","title":"LRP","text":"","category":"section"},{"location":"api/#Rules","page":"API Reference","title":"Rules","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"ZeroRule\nEpsilonRule\nGammaRule\nWSquareRule\nAlphaBetaRule\nFlatRule\nZBoxRule\nPassRule","category":"page"},{"location":"api/#ExplainableAI.ZeroRule","page":"API Reference","title":"ExplainableAI.ZeroRule","text":"ZeroRule()\n\nLRP-0 rule. Commonly used on upper layers.\n\nReferences\n\n[1]: S. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by     Layer-Wise Relevance Propagation\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.EpsilonRule","page":"API Reference","title":"ExplainableAI.EpsilonRule","text":"EpsilonRule([ϵ=1.0f-6])\n\nLRP-ϵ rule. Commonly used on middle layers.\n\nArguments:\n\nϵ: Optional stabilization parameter, defaults to 1f-6.\n\nReferences\n\n[1]: S. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by     Layer-Wise Relevance Propagation\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.GammaRule","page":"API Reference","title":"ExplainableAI.GammaRule","text":"GammaRule([γ=0.25])\n\nLRP-γ rule. Commonly used on lower layers.\n\nArguments:\n\nγ: Optional multiplier for added positive weights, defaults to 0.25.\n\nReferences\n\n[1]: G. Montavon et al., Layer-Wise Relevance Propagation: An Overview\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.WSquareRule","page":"API Reference","title":"ExplainableAI.WSquareRule","text":"WSquareRule()\n\nLRP-W^2 rule. Commonly used on the first layer when values are unbounded.\n\nReferences\n\n[1]: G. Montavon et al., Explaining nonlinear classification decisions with deep Taylor decomposition\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.AlphaBetaRule","page":"API Reference","title":"ExplainableAI.AlphaBetaRule","text":"AlphaBetaRule(alpha, beta)\nAlphaBetaRule([alpha=2.0], [beta=1.0])\n\nLRP-lphaeta rule. Weights positive and negative contributions according to the parameters alpha and beta respectively. The difference alpha - beta must be equal one. Commonly used on lower layers.\n\nArguments:\n\nalpha: Multiplier for the positive output term, defaults to 2.0.\nbeta: Multiplier for the negative output term, defaults to 1.0.\n\nReferences\n\n[1]: S. Bach et al., On Pixel-Wise Explanations for Non-Linear Classifier Decisions by     Layer-Wise Relevance Propagation [2]: G. Montavon et al., Layer-Wise Relevance Propagation: An Overview\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.FlatRule","page":"API Reference","title":"ExplainableAI.FlatRule","text":"FlatRule()\n\nLRP-Flat rule. Similar to the WSquareRule, but with all parameters set to one.\n\nReferences\n\n[1]: S. Lapuschkin et al., Unmasking Clever Hans predictors and assessing what machines really learn\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.ZBoxRule","page":"API Reference","title":"ExplainableAI.ZBoxRule","text":"ZBoxRule(low, high)\n\nLRP-z^mathcalB-rule. Commonly used on the first layer for pixel input.\n\nThe parameters low and high should be set to the lower and upper bounds of the input features, e.g. 0.0 and 1.0 for raw image data. It is also possible to provide two arrays of that match the input size.\n\nReferences\n\n[1]: G. Montavon et al., Explaining nonlinear classification decisions with deep Taylor decomposition\n\n\n\n\n\n","category":"type"},{"location":"api/#ExplainableAI.PassRule","page":"API Reference","title":"ExplainableAI.PassRule","text":"PassRule()\n\nPass-through rule. Passes relevance through to the lower layer. Supports reshaping layers.\n\n\n\n\n\n","category":"type"},{"location":"api/#Custom-rules","page":"API Reference","title":"Custom rules","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"These utilities can be used to define custom rules without writing boilerplate code:","category":"page"},{"location":"api/","page":"API Reference","title":"API Reference","text":"modify_input\nmodify_denominator\nmodify_param!\nmodify_layer!\ncheck_compat\nLRP_CONFIG.supports_layer\nLRP_CONFIG.supports_activation","category":"page"},{"location":"api/#ExplainableAI.modify_input","page":"API Reference","title":"ExplainableAI.modify_input","text":"modify_input(rule, input)\n\nModify input activation before computing relevance propagation.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.modify_denominator","page":"API Reference","title":"ExplainableAI.modify_denominator","text":"modify_denominator(rule, d)\n\nModify denominator z for numerical stability on the forward pass.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.modify_param!","page":"API Reference","title":"ExplainableAI.modify_param!","text":"modify_param!(rule, W)\nmodify_param!(rule, b)\n\nInplace-modify parameters before computing the relevance.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.modify_layer!","page":"API Reference","title":"ExplainableAI.modify_layer!","text":"modify_layer!(rule, layer; ignore_bias=false)\n\nIn-place modify layer parameters by calling modify_param! before computing relevance propagation.\n\nNote\n\nWhen implementing a custom modify_layer! function, modify_param! will not be called.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.LRP_CONFIG.supports_layer","page":"API Reference","title":"ExplainableAI.LRP_CONFIG.supports_layer","text":"LRP_CONFIG.supports_layer(layer)\n\nCheck whether LRP can be used on a layer or a Chain. To extend LRP to your own layers, define:\n\nLRP_CONFIG.supports_layer(::MyLayer) = true          # for structs\nLRP_CONFIG.supports_layer(::typeof(mylayer)) = true  # for functions\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.LRP_CONFIG.supports_activation","page":"API Reference","title":"ExplainableAI.LRP_CONFIG.supports_activation","text":"LRP_CONFIG.supports_activation(σ)\n\nCheck whether LRP can be used on a given activation function. To extend LRP to your own activation functions, define:\n\nLRP_CONFIG.supports_activation(::typeof(myactivation)) = true  # for functions\nLRP_CONFIG.supports_activation(::MyActivation) = true          # for structs\n\n\n\n\n\n","category":"function"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"strip_softmax\nflatten_model\ncanonize","category":"page"},{"location":"api/#ExplainableAI.strip_softmax","page":"API Reference","title":"ExplainableAI.strip_softmax","text":"strip_softmax(model)\n\nRemove softmax activation on model output if it exists.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.flatten_model","page":"API Reference","title":"ExplainableAI.flatten_model","text":"flatten_model(c)\n\nFlatten a Flux chain containing Flux chains.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExplainableAI.canonize","page":"API Reference","title":"ExplainableAI.canonize","text":"canonize(model)\n\nCanonize model by flattening it and fusing BatchNorm layers into preceding Dense and Conv layers with linear activation functions.\n\n\n\n\n\n","category":"function"},{"location":"api/#Index","page":"API Reference","title":"Index","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"EditURL = \"https://github.com/adrhill/ExplainableAI.jl/blob/master/docs/literate/advanced_lrp.jl\"","category":"page"},{"location":"generated/advanced_lrp/#Advanced-LRP-usage","page":"Advanced LRP","title":"Advanced LRP usage","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"One of the design goals of ExplainableAI.jl is to combine ease of use and extensibility for the purpose of research.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"This example will show you how to implement custom LRP rules and register custom layers and activation functions. For this purpose, we will quickly load our model from the previous section:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"using ExplainableAI\nusing Flux\nusing MLDatasets\nusing ImageCore\nusing BSON\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model]\n\nindex = 10\nx, _ = MNIST(Float32, :test)[10]\ninput = reshape(x, 28, 28, 1, :);\nnothing #hide","category":"page"},{"location":"generated/advanced_lrp/#Custom-LRP-composites","page":"Advanced LRP","title":"Custom LRP composites","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Instead of creating an LRP-analyzer from a single rule (e.g. LRP(model, GammaRule())), we can also assign rules to each layer individually. For this purpose, we create an array of rules that matches the length of the Flux chain:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"rules = [\n    ZBoxRule(0.0f0, 1.0f0),\n    EpsilonRule(),\n    GammaRule(),\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\n\nanalyzer = LRP(model, rules)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Since some Flux Chains contain other Flux Chains, ExplainableAI provides a utility function called flatten_model.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"warning: Flattening models\nNot all models can be flattened, e.g. those using Parallel and SkipConnection layers.","category":"page"},{"location":"generated/advanced_lrp/#Custom-LRP-rules","page":"Advanced LRP","title":"Custom LRP rules","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Let's define a rule that modifies the weights and biases of our layer on the forward pass. The rule has to be of type AbstractLRPRule.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"struct MyGammaRule <: AbstractLRPRule end","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"It is then possible to dispatch on the utility functions modify_input, modify_param! and modify_denominator with the rule type MyCustomLRPRule to define custom rules without writing any boilerplate code. To extend internal functions, import them explicitly:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"import ExplainableAI: modify_param!\n\nfunction modify_param!(::MyGammaRule, param)\n    param .+= 0.25 * relu.(param)\n    return nothing\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"We can directly use this rule to make an analyzer!","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"rules = [\n    ZBoxRule(0.0f0, 1.0f0),\n    EpsilonRule(),\n    MyGammaRule(),\n    EpsilonRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n    ZeroRule(),\n]\nanalyzer = LRP(model, rules)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"We just implemented our own version of the γ-rule in 4 lines of code. The heatmap perfectly matches the previous one!","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"If the layer doesn't use weights layer.weight and biases layer.bias, ExplainableAI provides a lower-level variant of modify_param! called modify_layer!. This function is expected to take a layer and return a new, modified layer. To add compatibility checks between rule and layer types, extend check_compat.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"warning: Using modify_layer!\nUse of the function modify_layer! will overwrite functionality of modify_param! for the implemented combination of rule and layer types. This is due to the fact that internally, modify_param! is called by the default implementation of modify_layer!.Therefore it is recommended to only extend modify_layer! for a specific rule and a specific layer type.","category":"page"},{"location":"generated/advanced_lrp/#Custom-layers-and-activation-functions","page":"Advanced LRP","title":"Custom layers and activation functions","text":"","category":"section"},{"location":"generated/advanced_lrp/#Model-checks-for-humans","page":"Advanced LRP","title":"Model checks for humans","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Good model checks and presets should allow novice users to apply XAI methods in a \"plug & play\" manner according to best practices.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Let's say we define a layer that doubles its input:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"struct MyDoublingLayer end\n(::MyDoublingLayer)(x) = 2 * x\n\nmylayer = MyDoublingLayer()\nmylayer([1, 2, 3])","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Let's append this layer to our model:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"model = Chain(model..., MyDoublingLayer())","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Creating an LRP analyzer, e.g. LRP(model), will throw an ArgumentError and print a summary of the model check in the REPL:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"┌───┬───────────────────────┬─────────────────┬────────────┬────────────────┐\n│   │ Layer                 │ Layer supported │ Activation │ Act. supported │\n├───┼───────────────────────┼─────────────────┼────────────┼────────────────┤\n│ 1 │ flatten               │            true │     —      │           true │\n│ 2 │ Dense(784, 100, relu) │            true │    relu    │           true │\n│ 3 │ Dense(100, 10)        │            true │  identity  │           true │\n│ 4 │ MyDoublingLayer()     │           false │     —      │           true │\n└───┴───────────────────────┴─────────────────┴────────────┴────────────────┘\n  Layers failed model check\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found unknown layers MyDoublingLayer() that are not supported by ExplainableAI's LRP implementation yet.\n\n  If you think the missing layer should be supported by default, please submit an issue (https://github.com/adrhill/ExplainableAI.jl/issues).\n\n  These model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument skip_checks=true.\n\n  [...]","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"LRP should only be used on deep rectifier networks and ExplainableAI doesn't recognize MyDoublingLayer as a compatible layer. By default, it will therefore return an error and a model check summary instead of returning an incorrect explanation.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"However, if we know MyDoublingLayer is compatible with deep rectifier networks, we can register it to tell ExplainableAI that it is ok to use. This will be shown in the following section.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"warning: Skipping model checks\nAll model checks can be skipped at the user's own risk by setting the LRP-analyzer keyword argument skip_checks=true.","category":"page"},{"location":"generated/advanced_lrp/#Registering-custom-layers","page":"Advanced LRP","title":"Registering custom layers","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The error in the model check will stop after registering our custom layer type MyDoublingLayer as \"supported\" by ExplainableAI.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"This is done using the function LRP_CONFIG.supports_layer, which should be set to return true for the type MyDoublingLayer:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"LRP_CONFIG.supports_layer(::MyDoublingLayer) = true","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Now we can create and run an analyzer without getting an error:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"analyzer = LRP(model)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"note: Registering functions\nFlux's Chains can also contain functions, e.g. flatten. This kind of layer can be registered asLRP_CONFIG.supports_layer(::typeof(mylayer)) = true","category":"page"},{"location":"generated/advanced_lrp/#Registering-activation-functions","page":"Advanced LRP","title":"Registering activation functions","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The mechanism for registering custom activation functions is analogous to that of custom layers:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"myrelu(x) = max.(0, x)\nmodel = Chain(Flux.flatten, Dense(784, 100, myrelu), Dense(100, 10))","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Once again, creating an LRP analyzer for this model will throw an ArgumentError and display the following model check summary:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"julia> analyzer = LRP(model3)\n┌───┬─────────────────────────┬─────────────────┬────────────┬────────────────┐\n│   │ Layer                   │ Layer supported │ Activation │ Act. supported │\n├───┼─────────────────────────┼─────────────────┼────────────┼────────────────┤\n│ 1 │ flatten                 │            true │     —      │           true │\n│ 2 │ Dense(784, 100, myrelu) │            true │   myrelu   │          false │\n│ 3 │ Dense(100, 10)          │            true │  identity  │           true │\n└───┴─────────────────────────┴─────────────────┴────────────┴────────────────┘\n  Activations failed model check\n  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\n\n  Found layers with unknown or unsupported activation functions myrelu. LRP assumes that the model is a \"deep rectifier network\" that only contains ReLU-like activation functions.\n\n  If you think the missing activation function should be supported by default, please submit an issue (https://github.com/adrhill/ExplainableAI.jl/issues).\n\n  These model checks can be skipped at your own risk by setting the LRP-analyzer keyword argument skip_checks=true.\n\n  [...]","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Registation works by defining the function LRP_CONFIG.supports_activation as true:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"LRP_CONFIG.supports_activation(::typeof(myrelu)) = true","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"now the analyzer can be created without error:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"analyzer = LRP(model)","category":"page"},{"location":"generated/advanced_lrp/#How-it-works-internally","page":"Advanced LRP","title":"How it works internally","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Internally, ExplainableAI dispatches to low level functions","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"lrp!(Rₖ, rule, layer, aₖ, Rₖ₊₁)\n    Rₖ .= ...\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"These functions in-place modify a pre-allocated array of the input relevance Rₖ (the ! is a naming convention in Julia to denote functions that modify their arguments).","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The correct rule is applied via multiple dispatch on the types of the arguments rule and layer. The relevance Rₖ is then computed based on the input activation aₖ and the output relevance Rₖ₊₁. Multiple dispatch is also used to dispatch modify_param! and modify_denominator on the rule and layer type.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Calling analyze on a LRP-model applies a forward-pass of the model, keeping track of the activations aₖ for each layer k. The relevance Rₖ₊₁ is then set to the output neuron activation and the rules are applied in a backward-pass over the model layers and previous activations.","category":"page"},{"location":"generated/advanced_lrp/#Generic-rule-implementation-using-automatic-differentiation","page":"Advanced LRP","title":"Generic rule implementation using automatic differentiation","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The generic LRP rule–of which the 0-, epsilon- and gamma-rules are special cases–reads[1][2]:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"R_j=sum_k fraca_j cdot rholeft(w_j kright)epsilon+sum_0 j a_j cdot rholeft(w_j kright) R_k","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"where rho is a function that modifies parameters – what we call modify_param!.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The computation of this propagation rule can be decomposed into four steps:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"beginarraylr\nforall_k z_k=epsilon+sum_0 j a_j cdot rholeft(w_j kright)  text  (forward pass)  \nforall_k s_k=R_k  z_k  text  (element-wise division)  \nforall_j c_j=sum_k rholeft(w_j kright) cdot s_k  text  (backward pass)  \nforall_j R_j=a_j c_j  text  (element-wise product) \nendarray","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"For deep rectifier networks, the third step can also be written as the gradient computation","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"c_j=leftnablaleft(sum_k z_k(boldsymbola) cdot s_kright)right_j","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"and can be implemented via automatic differentiation (AD).","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"This equation is implemented in ExplainableAI as the default method for all layer types that don't have a specialized implementation. We will refer to it as the \"AD fallback\".","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"[1]: G. Montavon et al., Layer-Wise Relevance Propagation: An Overview","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"[2]: W. Samek et al., Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications","category":"page"},{"location":"generated/advanced_lrp/#AD-fallback","page":"Advanced LRP","title":"AD fallback","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"The default LRP fallback for unknown layers uses AD via Zygote. For lrp!, we implement the previous four step computation using Zygote.pullback to compute c from the previous equation as a VJP, pulling back s_k=R_kz_k:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(Rₖ, rule, layer, aₖ, Rₖ₊₁)\n   check_compat(rule, layer)\n   reset! = get_layer_resetter(layer)\n   modify_layer!(rule, layer)\n   ãₖ₊₁, pullback = Zygote.pullback(layer, modify_input(rule, aₖ))\n   Rₖ .= aₖ .* only(pullback(Rₖ₊₁ ./ modify_denominator(rule, ãₖ₊₁)))\n   reset!()\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"You can see how check_compat, modify_layer!, modify_input and modify_denominator dispatch on the rule and layer type. This is how we implemented our own MyGammaRule. Unknown layers that are registered in the LRP_CONFIG use this exact function.","category":"page"},{"location":"generated/advanced_lrp/#Specialized-implementations","page":"Advanced LRP","title":"Specialized implementations","text":"","category":"section"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"We can also implement specialized versions of lrp! based on the type of layer, e.g. reshaping layers.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Reshaping layers don't affect attributions. We can therefore avoid the computational overhead of AD by writing a specialized implementation that simply reshapes back:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(Rₖ, rule, ::ReshapingLayer, aₖ, Rₖ₊₁)\n    Rₖ .= reshape(Rₖ₊₁, size(aₖ))\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"Since the rule type didn't matter in this case, we didn't specify it.","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"We can even implement the generic rule as a specialized implementation for Dense layers:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(Rₖ, rule, layer::Dense, aₖ, Rₖ₊₁)\n    reset! = get_layer_resetter(rule, layer)\n    modify_layer!(rule, layer)\n    ãₖ₊₁ = modify_denominator(rule, layer(modify_input(rule, aₖ)))\n    @tullio Rₖ[j, b] = aₖ[j, b] * layer.weight[k, j] * Rₖ₊₁[k, b] / ãₖ₊₁[k, b] # Tullio ≈ fast einsum\n    reset!()\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"For maximum low-level control beyond modify_layer!, modify_param! and modify_denominator, you can also implement your own lrp! function and dispatch on individual rule types MyRule and layer types MyLayer:","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"function lrp!(Rₖ, rule::MyRule, layer::MyLayer, aₖ, Rₖ₊₁)\n    Rₖ .= ...\nend","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"","category":"page"},{"location":"generated/advanced_lrp/","page":"Advanced LRP","title":"Advanced LRP","text":"This page was generated using Literate.jl.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"EditURL = \"https://github.com/adrhill/ExplainableAI.jl/blob/master/docs/literate/example.jl\"","category":"page"},{"location":"generated/example/#Getting-started","page":"Getting started","title":"Getting started","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"ExplainableAI.jl can be used on any classifier. In this first example, we will look at attributions on a LeNet5 model that was pretrained on MNIST.","category":"page"},{"location":"generated/example/#Loading-the-model","page":"Getting started","title":"Loading the model","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"note: Note\nOutside of these docs, you should be able to load the model usingusing BSON: @load\n@load \"model.bson\" model","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"using ExplainableAI\nusing Flux\nusing BSON\n\nmodel = BSON.load(\"../model.bson\", @__MODULE__)[:model]","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"warning: Strip softmax\nFor models with softmax activations on the output, it is necessary to callmodel = strip_softmax(model)before analyzing.","category":"page"},{"location":"generated/example/#Loading-MNIST","page":"Getting started","title":"Loading MNIST","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"We use MLDatasets to load a single image from the MNIST dataset:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"using MLDatasets\nusing ImageCore\nusing ImageIO\nusing ImageShow\n\nindex = 10\nx, _ = MNIST(Float32, :test)[10]\n\nconvert2image(MNIST, x)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"By convention in Flux.jl, this input needs to be resized to WHCN format by adding a color channel and batch dimensions.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"input = reshape(x, 28, 28, 1, :);\nnothing #hide","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"warning: Input format\nFor any attribution of a model, ExplainableAI.jl assumes the batch dimension to be come last in the input.For the purpose of heatmapping, the input is assumed to be in WHCN order (width, height, channels, batch), which is Flux.jl's convention.","category":"page"},{"location":"generated/example/#Calling-the-analyzer","page":"Getting started","title":"Calling the analyzer","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"We can now select an analyzer of our choice and call analyze to get an Explanation:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"analyzer = LRP(model)\nexpl = analyze(input, analyzer);\nnothing #hide","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This Explanation bundles the following data:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"expl.attribution: the analyzer's attribution\nexpl.output: the model output\nexpl.neuron_selection: the neuron index of used for the attribution\nexpl.analyzer: a symbol corresponding the used analyzer, e.g. :LRP","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Finally, we can visualize the Explanation through heatmapping:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"heatmap(expl)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Or get the same result by combining both analysis and heatmapping into one step:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"heatmap(input, analyzer)","category":"page"},{"location":"generated/example/#Neuron-selection","page":"Getting started","title":"Neuron selection","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"By passing an additional index to our call to analyze, we can compute the attribution with respect to a specific output neuron. Let's see why the output wasn't interpreted as a 4 (output neuron at index 5)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"heatmap(input, analyzer, 5)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This heatmap shows us that the \"upper loop\" of the hand-drawn 9 has negative relevance with respect to the output neuron corresponding to digit 4!","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"note: Note\nThe ouput neuron can also be specified when calling analyze:expl = analyze(img, analyzer, 5)","category":"page"},{"location":"generated/example/#Input-batches","page":"Getting started","title":"Input batches","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"ExplainableAI also supports input batches:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"batchsize = 100\nxs, _ = MNIST(Float32, :test)[1:batchsize]\nbatch = reshape(xs, 28, 28, 1, :) # reshape to WHCN format\nexpl_batch = analyze(batch, analyzer);\nnothing #hide","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This will once again return a single Explanation expl_batch for the entire batch. Calling heatmap on expl_batch will detect the batch dimension and return a vector of heatmaps.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Let's check if the digit at index = 10 still matches.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"hs = heatmap(expl_batch)\nhs[index]","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"JuliaImages' mosaic can be used to return a tiled view of all heatmaps:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"mosaic(hs; nrow=10)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"We can also evaluate a batch with respect to a specific output neuron, e.g. for the digit zero at index 1:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"mosaic(heatmap(batch, analyzer, 1); nrow=10)","category":"page"},{"location":"generated/example/#Automatic-heatmap-presets","page":"Getting started","title":"Automatic heatmap presets","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Currently, the following analyzers are implemented:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"├── Gradient\n├── InputTimesGradient\n├── SmoothGrad\n├── IntegratedGradients\n└── LRP\n    ├── LRPZero\n    ├── LRPEpsilon\n    └── LRPGamma","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Let's try InputTimesGradient","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"analyzer = InputTimesGradient(model)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"and Gradient","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"analyzer = Gradient(model)\nheatmap(input, analyzer)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"As you can see, the function heatmap automatically applies common presets for each method.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"Since InputTimesGradient and LRP both compute attributions, their presets are similar. Gradient methods however are typically shown in grayscale.","category":"page"},{"location":"generated/example/#Custom-heatmap-settings","page":"Getting started","title":"Custom heatmap settings","text":"","category":"section"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"We can partially or fully override presets by passing keyword arguments to heatmap:","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"using ColorSchemes\nheatmap(expl; cs=ColorSchemes.jet)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"heatmap(expl; reduce=:sum, rangescale=:extrema, cs=ColorSchemes.inferno)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This also works with batches","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"mosaic(heatmap(expl_batch; rangescale=:extrema, cs=ColorSchemes.inferno); nrow=10)","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"For the full list of keyword arguments, refer to the heatmap documentation.","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"","category":"page"},{"location":"generated/example/","page":"Getting started","title":"Getting started","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ExplainableAI","category":"page"},{"location":"#ExplainableAI.jl","page":"Home","title":"ExplainableAI.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Explainable AI in Julia using Flux.jl.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To install this package and its dependencies, open the Julia REPL and run ","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add ExplainableAI","category":"page"},{"location":"#Manual","page":"Home","title":"Manual","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\n    \"generated/example.md\",\n    \"generated/advanced_lrp.md\",\n]\nDepth = 2","category":"page"},{"location":"#API-reference","page":"Home","title":"API reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"api.md\",]\nDepth = 2","category":"page"}]
}
