{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic usage of LRP\n",
    "This example will show you best practices for using LRP,\n",
    "building on the basics shown in the *Getting started* section."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We start out by loading a small convolutional neural network:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using ExplainableAI\n",
    "using Flux\n",
    "\n",
    "model = Chain(\n",
    "    Chain(\n",
    "        Conv((3, 3), 3 => 8, relu; pad=1),\n",
    "        Conv((3, 3), 8 => 8, relu; pad=1),\n",
    "        MaxPool((2, 2)),\n",
    "        Conv((3, 3), 8 => 16; pad=1),\n",
    "        BatchNorm(16, relu),\n",
    "        Conv((3, 3), 16 => 8, relu; pad=1),\n",
    "        BatchNorm(8, relu),\n",
    "    ),\n",
    "    Chain(\n",
    "        Flux.flatten,\n",
    "        Dense(2048 => 512, relu),\n",
    "        Dropout(0.5),\n",
    "        Dense(512 => 100, softmax)\n",
    "    ),\n",
    ");"
   ],
   "metadata": {},
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "This model contains two chains: the convolutional layers and the fully connected layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model preparation\n",
    "### Stripping the output softmax\n",
    "When using LRP, it is recommended to explain output logits instead of probabilities.\n",
    "This can be done by stripping the output softmax activation from the model\n",
    "using the `strip_softmax` function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Chain(\n    Conv((3, 3), 3 => 8, relu, pad=1),  \u001b[90m# 224 parameters\u001b[39m\n    Conv((3, 3), 8 => 8, relu, pad=1),  \u001b[90m# 584 parameters\u001b[39m\n    MaxPool((2, 2)),\n    Conv((3, 3), 8 => 16, pad=1),       \u001b[90m# 1_168 parameters\u001b[39m\n    BatchNorm(16, relu),                \u001b[90m# 32 parameters\u001b[39m\u001b[90m, plus 32\u001b[39m\n    Conv((3, 3), 16 => 8, relu, pad=1),  \u001b[90m# 1_160 parameters\u001b[39m\n    BatchNorm(8, relu),                 \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  ),\n  Chain(\n    Flux.flatten,\n    Dense(2048 => 512, relu),           \u001b[90m# 1_049_088 parameters\u001b[39m\n    Dropout(0.5),\n    Dense(512 => 100),                  \u001b[90m# 51_300 parameters\u001b[39m\n  ),\n) \u001b[90m        # Total: 16 trainable arrays, \u001b[39m1_103_572 parameters,\n\u001b[90m          # plus 4 non-trainable, 48 parameters, summarysize \u001b[39m4.213 MiB."
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "cell_type": "code",
   "source": [
    "model = strip_softmax(model)"
   ],
   "metadata": {},
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you don't remove the output softmax,\n",
    "model checks will fail."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Canonizing the model\n",
    "LRP is not invariant to a model's implementation.\n",
    "Applying the `GammaRule` to two linear layers in a row will yield different results\n",
    "than first fusing the two layers into one linear layer and then applying the rule.\n",
    "This fusing is called \"canonization\" and can be done using the `canonize` function:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),   \u001b[90m# 1_168 parameters\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 14 trainable arrays, \u001b[39m1_103_540 parameters,\n\u001b[90m          # plus 2 non-trainable, 16 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "cell_type": "code",
   "source": [
    "model = canonize(model)"
   ],
   "metadata": {},
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Flattening the model\n",
    "ExplainableAI.jl's LRP implementation supports nested Flux Chains and Parallel layers.\n",
    "However, it is recommended to flatten the model before analyzing it.\n",
    "\n",
    "LRP is implemented by first running a forward pass through the model,\n",
    "keeping track of the intermediate activations, followed by a backward pass\n",
    "that computes the relevances.\n",
    "\n",
    "To keep the LRP implementation simple and maintainable,\n",
    "ExplainableAI.jl does not pre-compute \"nested\" activations.\n",
    "Instead, for every internal chain, a new forward pass is run to compute activations.\n",
    "\n",
    "By \"flattening\" a model, this overhead can be avoided.\n",
    "For this purpose, ExplainableAI.jl provides the function `flatten_model`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),   \u001b[90m# 1_168 parameters\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 14 trainable arrays, \u001b[39m1_103_540 parameters,\n\u001b[90m          # plus 2 non-trainable, 16 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "cell_type": "code",
   "source": [
    "model_flat = flatten_model(model)"
   ],
   "metadata": {},
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function is called by default when creating an LRP analyzer.\n",
    "Note that we pass the unflattened model to the analyzer, but `analyzer.model` is flattened:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Chain(\n  Conv((3, 3), 3 => 8, relu, pad=1),    \u001b[90m# 224 parameters\u001b[39m\n  Conv((3, 3), 8 => 8, relu, pad=1),    \u001b[90m# 584 parameters\u001b[39m\n  MaxPool((2, 2)),\n  Conv((3, 3), 8 => 16, relu, pad=1),   \u001b[90m# 1_168 parameters\u001b[39m\n  Conv((3, 3), 16 => 8, relu, pad=1),   \u001b[90m# 1_160 parameters\u001b[39m\n  BatchNorm(8, relu),                   \u001b[90m# 16 parameters\u001b[39m\u001b[90m, plus 16\u001b[39m\n  Flux.flatten,\n  Dense(2048 => 512, relu),             \u001b[90m# 1_049_088 parameters\u001b[39m\n  Dropout(0.5),\n  Dense(512 => 100),                    \u001b[90m# 51_300 parameters\u001b[39m\n) \u001b[90m        # Total: 14 trainable arrays, \u001b[39m1_103_540 parameters,\n\u001b[90m          # plus 2 non-trainable, 16 parameters, summarysize \u001b[39m4.212 MiB."
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "cell_type": "code",
   "source": [
    "analyzer = LRP(model)\n",
    "analyzer.model"
   ],
   "metadata": {},
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "If this flattening is not desired, it can be disabled\n",
    "by passing the keyword argument `flatten=false` to the `LRP` constructor."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LRP rules\n",
    "By default, the `LRP` constructor will assign the `ZeroRule` to all layers."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  MaxPool((2, 2))                   \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 16, relu, pad=1)\u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 16 => 8, relu, pad=1)\u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  BatchNorm(8, relu)                \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Flux.flatten                      \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dense(2048 => 512, relu)          \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dropout(0.5)                      \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Dense(512 => 100)                 \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "cell_type": "code",
   "source": [
    "LRP(model)"
   ],
   "metadata": {},
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "This analyzer will return heatmaps that look identical to `InputTimesGradient`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "LRP's strength lies in assigning different rules to different layers,\n",
    "based on their functionality in the neural network[^1].\n",
    "ExplainableAI.jl implements many LRP rules out of the box,\n",
    "but it is also possible to *implement custom rules*.\n",
    "\n",
    "To assign different rules to different layers,\n",
    "use one of the composites presets,\n",
    "or create your own composite, as described in\n",
    "*Assigning rules to layers*."
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Composite(\n  GlobalTypeMap(  \u001b[90m# all layers\u001b[39m\n\u001b[94m    Flux.Conv              \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.ConvTranspose     \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.CrossCor          \u001b[39m => \u001b[33mZPlusRule()\u001b[39m,\n\u001b[94m    Flux.Dense             \u001b[39m => \u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n\u001b[94m    typeof(NNlib.dropout)  \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.AlphaDropout      \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.Dropout           \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    Flux.BatchNorm         \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(Flux.flatten)   \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(MLUtils.flatten)\u001b[39m => \u001b[33mPassRule()\u001b[39m,\n\u001b[94m    typeof(identity)       \u001b[39m => \u001b[33mPassRule()\u001b[39m,\n ),\n  FirstLayerTypeMap(  \u001b[90m# first layer\u001b[39m\n\u001b[94m    Flux.Conv         \u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n\u001b[94m    Flux.ConvTranspose\u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n\u001b[94m    Flux.CrossCor     \u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n\u001b[94m    Flux.Dense        \u001b[39m => \u001b[33mFlatRule()\u001b[39m,\n ),\n)"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "cell_type": "code",
   "source": [
    "composite = EpsilonPlusFlat() # using composite preset EpsilonPlusFlat"
   ],
   "metadata": {},
   "execution_count": 7
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "LRP(\n  Conv((3, 3), 3 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mFlatRule()\u001b[39m,\n  Conv((3, 3), 8 => 8, relu, pad=1) \u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  MaxPool((2, 2))                   \u001b[90m => \u001b[39m\u001b[33mZeroRule()\u001b[39m,\n  Conv((3, 3), 8 => 16, relu, pad=1)\u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  Conv((3, 3), 16 => 8, relu, pad=1)\u001b[90m => \u001b[39m\u001b[33mZPlusRule()\u001b[39m,\n  BatchNorm(8, relu)                \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Flux.flatten                      \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Dense(2048 => 512, relu)          \u001b[90m => \u001b[39m\u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n  Dropout(0.5)                      \u001b[90m => \u001b[39m\u001b[33mPassRule()\u001b[39m,\n  Dense(512 => 100)                 \u001b[90m => \u001b[39m\u001b[33mEpsilonRule{Float32}(1.0f-6)\u001b[39m,\n)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "cell_type": "code",
   "source": [
    "LRP(model, composite)"
   ],
   "metadata": {},
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing layerwise relevances\n",
    "If you are interested in computing layerwise relevances,\n",
    "call `analyze` with an LRP analyzer and the keyword argument\n",
    "`layerwise_relevances=true`.\n",
    "\n",
    "The layerwise relevances can be accessed in the `extras` field\n",
    "of the returned `Explanation`:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "11-element Vector{Array{Float32}}:\n [0.11555797 0.16567013 … -0.00074588886 -0.016081242; -0.020463338 -0.18181762 … 0.0075527276 -0.0012225641; … ; 0.05978778 0.025775693 … 0.016010145 0.0058790017; 0.027305666 0.034312364 … 0.011098867 0.0031443986;;; 0.17919934 -0.44290525 … -0.0028781097 0.0025866567; -0.17921384 -1.1027005 … 0.00529778 0.014584399; … ; 0.0065630525 0.019256484 … 0.01691892 -0.017772272; -0.006193216 -0.0006287951 … -0.0018861903 0.0041949633;;; -0.23348749 -0.14968054 … -0.017748682 0.005836729; 0.22589077 -0.11296646 … 0.0004931396 0.008539669; … ; 0.0057462584 -0.0026442981 … -0.015646847 -0.0002237301; 0.0056847427 -0.013524553 … 0.01720984 -0.004970015;;;;]\n [-0.037589494 -0.35138637 … -0.0023543204 -0.0031317736; -0.32994968 -0.343098 … -0.0023740756 -0.001340456; … ; 0.001353058 -0.045696773 … 0.024047514 -0.0043757693; 0.009031263 0.00071556837 … -0.012437812 -0.0006313002;;; 0.12511042 0.16571468 … 0.0062297885 -0.0023807054; 0.032985922 0.043249875 … 0.002118163 0.004735875; … ; -0.0 -0.0030185538 … -0.018875849 0.0036812269; -0.0 0.007889737 … 0.017898919 0.009926329;;; -0.05125717 -0.0 … 0.0042070267 0.0; 0.09910777 0.105798446 … -0.0 0.0; … ; -0.010481375 0.009564088 … 0.012331664 0.0; -0.0 -0.0 … 0.0010695708 -4.6312893f-5;;; -0.10725487 -0.032024257 … 0.0032939261 0.0; 0.0 0.0 … -5.4200027f-5 -0.0011091139; … ; -0.0 0.0047965213 … -0.0 -0.0005881093; -0.0010443022 -0.00014615235 … 0.0 -0.0;;; -0.69584686 -1.0485102 … 0.027182447 0.0036318763; 0.19321147 0.038500734 … -0.023141088 0.004430523; … ; -0.00366669 -0.0002217274 … 0.03482189 0.00024875192; 0.0018984725 -0.010518859 … 0.0038209218 0.0;;; -0.025747867 0.30286813 … 0.00014623615 -0.0029056992; 0.06188767 0.004420979 … -0.010237755 -0.00016455087; … ; -0.00019923941 0.010801336 … 0.0066850116 0.0039750487; -0.010430547 0.0040452997 … 0.004033728 -0.0018882912;;; 0.0 -0.0 … -0.0 -0.0; -0.0 0.0 … -0.0 -0.0; … ; 0.0 0.0 … -0.0 -0.0; -0.0 0.0 … 0.0 0.0;;; 0.022411661 -0.0 … 0.0 -0.0037738564; 0.11818989 0.0 … 0.0012431474 -0.0019136093; … ; -0.00065092824 0.0 … 0.0 -0.0040879594; -0.00030009574 -0.001830498 … -0.0 -0.00044494044;;;;]\n [0.0 0.0 … -0.001536238 0.0; -0.32529083 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.00089153595 0.0 … 0.0067885933 0.0;;; 0.0 0.0076351017 … -0.01013882 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.00069445185; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0041311784; … ; 0.0 0.0 … 0.0 0.0; 0.0 -0.029274946 … -0.018185424 0.0;;; 0.0 0.0 … 0.0 -0.0012027454; 0.0 -0.8644419 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.006391116 … -0.00089663255 0.0;;; 0.0 -0.8371275 … 0.0 0.006528635; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.038002744 … 0.0 0.0; 0.0 0.0 … 0.0 -0.011889387;;; 0.0 0.0 … 0.0 -0.0031841923; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0043347296;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.001537811; 0.0 -0.0061070193 … 0.0 0.0;;; -0.2521484 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.04499356 … 0.042463027 0.0;;;;]\n [-0.32529083 0.072779745 … -0.016228346 -0.001536238; 0.2532575 -0.0 … 0.010171676 0.0009467099; … ; 0.027731834 0.22912401 … 0.03565193 -0.0; 0.00089153595 0.008775006 … 0.009476386 0.0067885933;;; 0.0076351026 0.031596325 … -0.0 -0.01013882; 0.0 0.03872561 … 0.0 0.009164122; … ; 0.0 0.0 … -0.0 0.0010779466; -0.0 -0.0006514026 … -0.0 0.00069445185;;; 0.0 0.21772698 … -0.0 0.0041311784; -0.0 -0.0 … -0.08997348 -0.019909158; … ; 0.0015084054 0.03941227 … -0.0 -0.012937454; -0.029274946 -0.026991209 … -0.03406386 -0.018185424;;; -0.8644419 0.10458824 … -0.013436062 -0.0012027454; 1.2725815 -0.2836974 … 0.004793112 -0.012660509; … ; -0.05805798 0.04653638 … 0.012597835 -0.0042927563; 0.006391116 0.048833884 … 0.004463303 -0.00089663255;;; -0.8371275 -0.12549062 … 0.020481162 0.006528635; 0.0 0.19743301 … -0.0055893334 0.07488632; … ; -0.023937635 -0.036739197 … -0.0034947726 0.04904119; 0.038002744 -0.05372891 … 0.00037613173 -0.011889387;;; 0.0 -0.0 … -0.00052843615 -0.0031841923; -0.0 0.0 … -0.013878753 -0.012208857; … ; 0.0 0.0 … -0.0 -0.0; -0.0 0.0006502942 … -0.0 0.0043347296;;; 0.0 -0.17113881 … 0.0 0.0; -0.6252986 0.0 … 0.0 -0.0035140316; … ; -0.0010301681 0.0 … 7.153189f-5 0.0067811585; -0.0061070193 0.0 … 0.0004158783 0.001537811;;; -0.2521484 -0.36160702 … -0.0025597953 0.0; -0.2626785 -0.031484623 … -0.018764477 -0.025711201; … ; 0.066031694 -0.09855567 … -0.018081622 -0.0066982876; 0.04499356 -0.035856098 … 0.037654907 0.042463027;;;;]\n [-0.07855032 0.00196009 … -0.0 -0.0; -0.014896327 -0.8878472 … -0.008299498 -0.0; … ; 0.04742264 -0.033691153 … -0.0 -0.0; -0.011322196 -0.10518424 … -0.007691069 -0.0018612122;;; -0.0 -0.024953386 … 0.0 0.0; -0.0 0.0 … 2.1948752f-5 0.0; … ; 0.0 -0.0 … 0.0 0.0; 0.0053180666 0.0 … 0.0 0.0;;; 0.0 -0.0 … 0.0 -0.0013910539; -0.0 0.0 … -0.0 0.0052974066; … ; -0.0 0.0 … 0.0 -0.0012707297; -0.0 0.0 … -0.0 -0.00069558783;;; … ;;; -0.0 -0.16272092 … 0.0 0.0; -0.10774095 -0.15646434 … -0.0 0.0; … ; 0.009053699 0.0 … 0.0 0.0; 0.008717909 -0.0020142347 … 0.0014557827 0.0;;; 0.0011598621 -0.772295 … 0.014580138 0.022462297; 0.42342895 0.9504279 … -0.010914816 0.010951316; … ; 0.00361706 0.11409792 … 0.012012022 0.023946112; 0.0 0.013503348 … 0.019456247 -0.012093022;;; -0.0 0.0029368615 … 0.00012988015 -0.0; -0.0 0.1753267 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; -0.0 0.0 … -0.0 0.0;;;;]\n [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.00025325536 -0.000578286 … 0.0 0.0; 0.00025790694 -0.00214369 … 0.0 0.0;;; -0.0021142424 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0;;; -0.0 -0.0 … -6.964325f-5 -3.2026877f-5; 0.0013377107 0.044989742 … -0.0 -8.142743f-5; … ; -0.0002638756 -0.036990084 … -0.012151355 0.008718809; -2.5924955f-5 -0.0 … -0.00012693179 -0.00030154805;;; 0.001276079 0.0012933273 … 0.00049700524 0.0; -5.7388935 -0.17388494 … 0.00066013855 0.0013053188; … ; -0.036486518 -0.032211166 … 0.00082350534 0.0013062144; -0.0010246906 -0.2589682 … -0.00032640586 0.0012392186;;; -0.02728919 -0.030647784 … 0.04076553 -0.0101187; -0.006137956 -0.0 … -0.052455477 -0.03686809; … ; -0.0013391115 -0.0 … -0.0 -0.0; -8.074782f-5 -0.0 … -0.0120127 -0.0;;; -0.0 -0.0 … -0.0016737665 -0.013131372; -0.0 -0.0 … -0.0 -0.021611648; … ; -0.0 -0.0076563214 … -0.01021521 -0.029008528; -0.0 -0.0 … 0.008279179 -0.001842284;;; 0.0 0.26381096 … -0.012868868 0.053802982; 0.0032877645 0.061142918 … 0.021163741 0.014161846; … ; 0.055578362 0.06774405 … 0.0713381 0.055979244; 0.008224021 0.066860475 … 0.01846212 0.0;;; -0.0 -0.0 … -0.0 -0.004081749; -0.0 -0.0 … -0.0 0.0024254376; … ; -0.0 -0.0 … -0.0 -3.81782f-5; -0.0012016319 -0.001447384 … -0.0 -0.008825191;;;;]\n [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.00019299799 -0.00069045747 … 0.0 0.0; 0.00019671685 -0.0018927953 … 0.0 0.0;;; -0.0022356662 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.00022334259 -0.00013145785; 0.00030036323 0.00062986766 … 0.0 0.0005030005; … ; -0.0016554995 -0.013122062 … -0.0007300087 0.005158066; -2.7746066f-5 0.0 … -0.00026150592 0.0010497526;;; -0.0015516146 -0.0023198463 … 0.0007291804 0.0; -0.016532248 -0.020140184 … 0.008194434 0.0053568445; … ; -0.008068854 -0.0022266698 … 0.0015820686 0.0055338885; -0.005404968 -0.035394073 … 0.0065475777 -0.006851507;;; -0.0015183646 0.005927388 … 0.008750405 -0.0047523314; 0.00058639195 0.0 … -0.0045847525 -0.000464695; … ; 0.00063990976 0.0 … 0.0 0.0; -2.6791107f-5 0.0 … 0.00079456065 0.0;;; 0.0 0.0 … 0.00036528043 0.00016716821; 0.0 0.0 … 0.0 -0.0024151024; … ; 0.0 0.0018217764 … 0.00064925384 -0.008731368; 0.0 0.0 … 0.0019635109 0.005983344;;; 0.0 0.003956399 … -0.0131952455 0.004792052; 0.00014768173 0.0063210945 … -0.00094137364 0.0039392924; … ; 0.0047478415 0.0077295173 … 0.009154586 0.0056523103; -0.0007516285 0.0034228517 … 0.0008037041 0.0;;; 0.0 0.0 … 0.0 -0.0027430686; 0.0 0.0 … 0.0 0.0027418195; … ; 0.0 0.0 … 0.0 0.0006357164; -0.00057337456 -0.0006643733 … 0.0 -0.006642742;;;;]\n [0.0; 0.0; … ; 0.00063571654; -0.006642742;;]\n [-0.0; 0.0; … ; -0.0; 0.0;;]\n [-0.011225047; 0.0; … ; -0.013638405; 0.0054359557;;]\n [0.0; 0.0; … ; 0.0; 0.0;;]"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "cell_type": "code",
   "source": [
    "input = rand(Float32, 32, 32, 3, 1) # dummy input for our convolutional neural network\n",
    "\n",
    "expl = analyze(input, analyzer; layerwise_relevances=true)\n",
    "expl.extras.layerwise_relevances"
   ],
   "metadata": {},
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the layerwise relevances are only kept for layers in the outermost `Chain` of the model.\n",
    "When using our unflattened model, we only obtain three layerwise relevances,\n",
    "one for each chain in the model and the output relevance:"
   ],
   "metadata": {}
  },
  {
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "11-element Vector{Array{Float32}}:\n [0.10909061 0.016985286 … -0.033392604 0.035766583; 0.0047575007 -0.10714021 … -0.012110931 0.0013750129; … ; -0.07479966 0.007444864 … 0.036861412 0.009906101; 0.00805714 -0.022324992 … 0.01221438 0.045526262;;; 0.1833335 -0.31012642 … 0.0042630946 -0.015650481; -0.101551116 -0.8807603 … 0.00069726567 -0.030251766; … ; -0.015744127 -0.03496587 … -0.015696604 -0.014254219; -0.023524927 -0.024451341 … -0.002061537 0.015031568;;; -0.12570041 -0.118935265 … 0.015687658 0.011928017; 0.14294061 -0.07356083 … 0.01841417 0.04019364; … ; 0.025564123 -0.013152767 … -0.009696721 -0.0025443526; -0.003473982 0.010800932 … 0.017462572 0.0029346414;;;;]\n [-0.036506597 -0.25065005 … 0.01619241 0.011507603; -0.22996363 -0.35731292 … -0.004716799 0.0038788142; … ; -0.033761796 0.0313675 … 0.022528403 -0.0051091155; -0.0072388975 -0.008173408 … 0.009247528 -0.0056583444;;; 0.08575319 0.17711058 … -0.007875867 0.00058436993; 0.013789675 0.09537185 … -0.0010806506 -0.0074940566; … ; 0.0 0.0029525734 … -0.02821751 -0.01039312; 0.0 -0.009143967 … 0.02028125 -0.003175292;;; -0.042887196 -0.0 … -0.004717435 -0.0; 0.060022302 0.1207603 … 0.0 0.0; … ; 0.0047387774 -0.008239751 … 0.013589981 0.0; 0.0 0.0 … -0.00071525894 0.0007352606;;; -0.06445834 -0.0585551 … -0.009763665 -0.0; 0.0 0.0 … 0.01818304 -0.0006693352; … ; 0.0 0.00562708 … -0.0 0.0024318378; 0.0021766252 -0.0004362477 … -0.0 0.0;;; -0.4763521 -0.8459456 … -0.02505148 -0.0030459946; 0.09090679 -0.0654049 … 0.0602672 -0.0027281241; … ; -0.0073197423 -0.016405828 … -0.025265057 -0.0061176745; -0.0004264298 0.006827222 … 0.030976454 0.0;;; -0.014431472 0.27200603 … -0.00022729569 -0.0038549162; 0.041840933 0.059507057 … -0.0012789028 -0.0013737421; … ; -0.02029101 0.0017885001 … -0.010666857 0.011415687; 0.0095414845 -0.017155882 … -0.001260215 -0.008891415;;; 0.0 -0.0 … 0.0 0.0; -0.0 0.0 … 0.0 0.0; … ; -0.0 -0.0 … 0.0 -0.0; 0.0 -0.0 … 0.0 0.0;;; 0.021060636 -0.0 … -0.0 0.021627596; 0.07137109 0.0 … 0.0025617837 -0.02132579; … ; -0.0022815014 0.0 … -0.0 0.0019612527; -0.012264215 -0.00527389 … 0.0 0.0140815275;;;;]\n [0.0 0.0 … 0.0249988 0.0; -0.26958176 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; -0.0032865105 0.0 … -0.008930933 0.0;;; 0.0 0.0020870604 … 0.018985491 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 -0.0013934255; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 -0.024125686; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.058032494 … 0.009138649 0.0;;; 0.0 0.0 … 0.0 0.001083702; 0.0 -0.56319964 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 -0.019358065 … 0.005610562 0.0;;; 0.0 -0.6203666 … 0.0 0.020886518; 0.0 0.0 … 0.0 0.0; … ; 0.0 -0.01582041 … 0.0 0.0; 0.0 0.0 … 0.0 0.06050062;;; 0.0 0.0 … 0.0 0.006584508; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 -0.011490189;;; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0015476055; 0.0 -0.0120370025 … 0.0 0.0;;; -0.069292426 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 -0.048977695 … 0.08440569 0.0;;;;]\n [-0.26958176 0.05057835 … 0.03289142 0.0249988; 0.20903519 -0.0 … 0.069433786 -0.021604175; … ; 0.01612467 -0.097033486 … 0.008083403 -0.0; -0.0032865105 0.055063967 … 0.020193087 -0.008930933;;; 0.0020870604 0.09518771 … 0.0 0.018985491; 0.0 0.08856021 … 0.0 0.12412661; … ; -0.0 -0.0 … -0.0 -0.021148896; 0.0 -0.015397718 … -0.0 -0.0013934255;;; 0.0 -0.047148593 … 0.0 -0.024125686; 0.0 -0.0 … -0.1029622 -0.02053965; … ; 0.0021314933 -0.059277765 … 0.0 0.0027859597; 0.058032494 -0.021531822 … -0.010623338 0.009138649;;; -0.56319964 0.045164842 … -0.05104338 0.0010837021; 0.6500704 -0.10057322 … 0.116284445 0.030583534; … ; 0.004337278 -0.08474632 … 0.009033818 -0.0033665996; -0.019358065 0.013887664 … 0.01867761 0.005610562;;; -0.6203666 0.057189662 … 0.13043272 0.020886518; -0.0 -0.2689995 … 0.0013740978 -0.18750094; … ; 0.032115858 -0.052761964 … -0.008885173 0.030270008; -0.01582041 0.107497804 … -0.010417068 0.06050062;;; 0.0 -0.0 … -1.1341947f-5 0.006584508; 0.0 0.0 … 0.005908674 0.063301444; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.00039320008 … 0.0 -0.011490189;;; 0.0 -0.1795146 … 0.0 0.0; -0.3742136 0.0 … -0.0 -0.0145936385; … ; 0.005261438 -0.0 … 0.00018893428 0.0073752846; -0.0120370025 -0.0 … 0.00022015258 0.0015476055;;; -0.069292426 -0.3425286 … 0.01214021 0.0; -0.20765051 -0.038671862 … -0.07501496 0.05664548; … ; 0.045021582 0.06282013 … 0.007421834 0.0027402488; -0.048977695 0.04136082 … 0.15190281 0.08440569;;;;]\n [-0.16807593 -0.00038939033 … 0.0 0.0; -0.01918672 -0.54489285 … 0.2542474 0.0; … ; 0.025190767 0.024571508 … 0.0 -0.0; 0.040931985 0.09611825 … -0.0021987807 0.0036025958;;; -0.0 -0.011272973 … 0.0 0.0; -0.0 0.0 … 0.0010476111 -0.0; … ; 0.0 0.0 … 0.0 0.0; 0.005500955 0.0 … 0.0 -0.0;;; 0.0 -0.0 … -0.0 0.0070097805; -0.0 0.0 … -0.0 -0.0605208; … ; 0.0 -0.0 … 0.0 0.021108273; 0.0 -0.0 … 0.0 0.00059921195;;; … ;;; -0.0 -0.09291314 … 0.0 0.0; -0.043849297 -0.09413411 … 0.0 -0.0; … ; -0.0037466194 -0.0 … 0.0 0.0; 0.03011647 0.024953123 … 0.0011174951 0.0;;; -0.00026625805 -0.47847658 … 0.029899087 -0.011348398; 0.37205762 0.5127711 … 0.0690732 -0.059880264; … ; -0.0010180731 0.023897378 … 0.032561462 -0.013875803; 0.0 -0.004059144 … 0.021860223 0.0030347547;;; -0.0 0.024330003 … 7.210141f-5 0.0; -0.0 0.18096678 … 0.0 -0.0; … ; 0.0 -0.0 … 0.0 -0.0; 0.0 -0.0 … -0.0 -0.0;;;;]\n [-0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0014693483 0.00087103667 … -0.0 -0.0; 0.000712205 0.0004526412 … -0.0 -0.0;;; -0.0032647247 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0; … ; -0.0 -0.0 … -0.0 -0.0; -0.0 -0.0 … -0.0 -0.0;;; -0.0 -0.0 … -6.9886f-5 -2.8113112f-5; 0.015341483 -0.4279785 … -0.0 -8.52097f-5; … ; -0.00057602755 -0.029404787 … 0.0009434741 -0.01466591; -2.2266046f-5 -0.0 … -0.00015644934 -0.0008297956;;; 0.0045853104 0.0052902126 … 0.0014710545 0.0; -3.879381 0.17541379 … 0.0057629375 0.0049753017; … ; -0.0023168456 -0.0061817425 … 0.00254286 0.0050499276; 0.004407694 -0.058885217 … 0.005088185 0.0055331453;;; 0.01664174 0.019312974 … -0.005006531 0.007520891; 0.004637208 0.0 … 0.045063864 0.012015342; … ; 0.0010321845 0.0 … 0.0 0.0; 6.257282f-5 0.0 … 0.008860406 0.0;;; 0.0 0.0 … 0.0050628604 0.027811034; 0.0 0.0 … 0.0 0.059925538; … ; 0.0 0.05243204 … 0.02179366 0.035637934; 0.0 0.0 … 0.03361498 0.04308124;;; 0.0 -0.046526916 … -0.0094169015 0.05705587; 0.002336602 0.019722909 … 0.015049898 0.010068342; … ; 0.029206987 0.050646834 … 0.05410663 0.08566312; 0.0058457106 0.13492039 … 0.013127517 0.0;;; -0.0 -0.0 … -0.0 -0.00043023916; -0.0 -0.0 … -0.0 0.0099360095; … ; -0.0 -0.0 … -0.0 -6.194052f-6; 0.004096542 0.006739668 … -0.0 -0.0022184034;;;;]\n [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; -0.00096417195 0.0025425456 … 0.0 0.0; 0.0007130215 0.0006981252 … 0.0 0.0;;; -0.0032050891 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0;;; 0.0 0.0 … 1.4059367f-5 0.0; 0.0029872751 -0.0059376317 … 0.0 9.514808f-5; … ; 0.0029633814 -0.009852298 … 0.00012038374 -0.0062369113; 0.0 0.0 … -0.0005627781 -0.0009423485;;; 0.002408507 -0.007942966 … 0.0 0.0; -0.011189836 0.020324802 … -0.004016214 0.0063461987; … ; -0.00035452866 -0.0006969604 … -0.00024563205 0.0068167793; 0.0042496896 -0.00844548 … -0.0020587514 -0.0029978447;;; 0.00039793408 -0.0044802655 … -0.0027069023 -0.004153416; 0.00039026883 0.0 … 0.009906187 -0.006654854; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0026402038 0.0;;; 0.0 0.0 … 0.0011237959 -0.007889131; 0.0 0.0 … 0.0 0.006470887; … ; 0.0 0.015827665 … -0.0074078036 -0.013098773; 0.0 0.0 … 0.0037569758 0.009128958;;; 0.0 -0.0010951807 … -0.009458962 0.010107953; 0.00017005035 -0.006325204 … 0.0037686406 0.0004751422; … ; 0.0011803741 0.0060589933 … 0.00826315 0.018243885; -0.001660798 0.010315852 … 0.002854667 0.0;;; 0.0 0.0 … 0.0 6.654783f-5; 0.0 0.0 … 0.0 0.009028647; … ; 0.0 0.0 … 0.0 0.0004901995; 0.0027190999 0.0037400024 … 0.0 -0.0011963211;;;;]\n [0.0; 0.0; … ; 0.00049019954; -0.0011963211;;]\n [-0.0; 0.0; … ; -0.013183771; 0.005037208;;]\n [-0.011272744; 0.0; … ; -0.013183771; 0.005037208;;]\n [0.0; 0.0; … ; 0.0; 0.0;;]"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "cell_type": "code",
   "source": [
    "analyzer = LRP(model; flatten=false) # use unflattened model\n",
    "\n",
    "expl = analyze(input, analyzer; layerwise_relevances=true)\n",
    "expl.extras.layerwise_relevances"
   ],
   "metadata": {},
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performance tips\n",
    "### Using LRP without a GPU\n",
    "Since ExplainableAI.jl's LRP implementation makes use of\n",
    "[Tullio.jl](https://github.com/mcabbott/Tullio.jl),\n",
    "analysis can be accelerated by loading either\n",
    "- a package from the [JuliaGPU](https://juliagpu.org) ecosystem,\n",
    "  e.g. [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl), if a GPU is available\n",
    "- [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "  if only a CPU is available.\n",
    "\n",
    "This only requires loading the LoopVectorization.jl package before ExplainableAI.jl:\n",
    "```julia\n",
    "using LoopVectorization\n",
    "using ExplainableAI\n",
    "```\n",
    "\n",
    "[^1]: G. Montavon et al., [Layer-Wise Relevance Propagation: An Overview](https://link.springer.com/chapter/10.1007/978-3-030-28954-6_10)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  },
  "kernelspec": {
   "name": "julia-1.9",
   "display_name": "Julia 1.9.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
